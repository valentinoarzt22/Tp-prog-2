---
title: "EDA: Datos metereol√≥gicos por estaciones"
output: html_notebook
---
# Introducci√≥n
Este documento presenta un An√°lisis Exploratorio de Datos (EDA) sobre un conjunto de datos meteorol√≥gicos horarios de m√∫ltiples estaciones en Argentina. El objetivo es limpiar, procesar y transformar los datos brutos para descubrir patrones, analizar tendencias temporales y geoespaciales, e identificar eventos extremos.

El an√°lisis comienza con una fase de ingesta y limpieza de datos, seguida de la ingenier√≠a de variables (como la desagregaci√≥n de precipitaci√≥n y la categorizaci√≥n de vientos), y concluye con una serie de visualizaciones interactivas y un modelado predictivo b√°sico

# 1. Configuraci√≥n e Ingesta de Datos

Primero, cargamos todas las librer√≠as necesarias para el an√°lisis, incluyendo tidyverse para la manipulaci√≥n de datos, leaflet para mapas, y prophet para series temporales.
```{r}
rm(list = ls())
options(scipen=999)

paquetes_requeridos <- c(
  "tidyverse", "slider", "rstudioapi", "arrow", "zoo", "data.table", 
  "janitor", "leaflet", "scales", "prophet", "htmlwidgets", "plotly", 
  "viridisLite", "png", "openair", "base64enc", "tibble", 
  "RColorBrewer", "stringi", "openair"
)
paquetes_a_instalar <- paquetes_requeridos[!(paquetes_requeridos %in% installed.packages()[,"Package"])]

if (length(paquetes_a_instalar) > 0) {
  install.packages(paquetes_a_instalar)
}

lapply(paquetes_requeridos, require, character.only = TRUE)
```

No tenemos el dataset completo. Hay que armarlo. Primero empezamos uniendo los archivos de texto, que estan separado por espacios tabulados. Identificamos cada columna con expresiones regulares.  Luego, convertimos las columnas de texto a sus tipos de datos correctos (num√©rico, fecha) y guardamos el resultado en dataset_clima_unificado.Rds para no tener que repetir este costoso proceso.

```{r}
# -----------------------------------------------------------------
# PASO 2: Definir la ubicaci√≥n de tus archivos
# -----------------------------------------------------------------
# Usa "." si ya estableciste el directorio de trabajo (recomendado)
ruta_carpeta <- "smn-data/"

# -----------------------------------------------------------------
# PASO 3: La funci√≥n de procesamiento (REGEX HIPER-REFINADO)
# -----------------------------------------------------------------
procesar_archivo_regex_general <- function(ruta_archivo) {
  
  print(paste("Procesando:", ruta_archivo)) # Progreso
  
  tryCatch({
    
    # --- 1. Leer l√≠neas (latin1) ---
    lineas <- readr::read_lines( ruta_archivo, skip_empty_rows = TRUE, locale = locale(encoding = "latin1") )
    
    # --- 2. Omitir archivos vac√≠os / solo encabezado ---
    if (is.null(lineas) || length(lineas) <= 2) { return(NULL) }
    lineas_datos_raw <- lineas[-(1:2)]
    if (length(lineas_datos_raw) == 0) { return(NULL) }
    
    # --- 3. L√≥gica de "Cosido" ---
    lineas_cosidas <- c(); buffer_linea <- ""
    for (linea_actual in lineas_datos_raw) {
      if (nchar(trimws(linea_actual)) == 0) { next }
      if (stringr::str_starts(trimws(linea_actual), "^\\d{8}")) {
        if (nchar(buffer_linea) > 0) { lineas_cosidas <- c(lineas_cosidas, buffer_linea) }
        buffer_linea <- linea_actual
      } else { buffer_linea <- paste0(buffer_linea, linea_actual) }
    }
    if (nchar(buffer_linea) > 0) { lineas_cosidas <- c(lineas_cosidas, buffer_linea) }
    
    # --- 4. Extracci√≥n con Expresiones Regulares (HIPER-REFINADO) ---
    # Patr√≥n m√°s espec√≠fico basado en tipos de datos esperados:
    patron_especifico <- paste0(
      "^",                # Inicio de l√≠nea
      "(\\d{8})",         # Grupo 1: FECHA (8 d√≠gitos exactos)
      "\\s+",             # Separador
      "(\\d+|\\s{1,})",   # Grupo 2: HORA (n√∫meros O espacios)
      "\\s+",             # Separador
      "([-+]?\\d*\\.?\\d+|\\s{1,})", # Grupo 3: TEMP (n√∫mero con decimal opcional O espacios)
      "\\s+",             # Separador
      "(\\d+|\\s{1,})",   # Grupo 4: HUM (n√∫meros O espacios)
      "\\s+",             # Separador
      # Grupo 5: PNM (n√∫mero con decimal opcional O AL MENOS DOS espacios)
      "([-+]?\\d+\\.\\d+|\\s{2,})",
      "\\s+",             # Separador
      "(\\d+|\\s{1,})",   # Grupo 6: DD (SOLO n√∫meros O espacios)
      "\\s+",             # Separador
      "(\\d+|\\s{1,})",   # Grupo 7: FF (SOLO n√∫meros O espacios)
      "\\s+",             # Separador
      "(.*?)",            # Grupo 8: NOMBRE (el resto, no goloso)
      "\\s*$"             # Espacios opcionales al final
    )
    
    matches <- str_match(lineas_cosidas, patron_especifico)
    
    # Verificaci√≥n
    if (is.null(matches) || ncol(matches) != 9 || all(is.na(matches[,2]))) {
      warning(paste("Regex espec√≠fico fall√≥ en:", ruta_archivo, "- Omitiendo."))
      # Podr√≠amos a√±adir un fallback aqu√≠ si fuera necesario, pero confiemos en este patr√≥n
      return(NULL)
    }
    
    # Convertir a data frame
    datos_extraidos <- as.data.frame(matches[, -1, drop = FALSE], stringsAsFactors = FALSE)
    colnames(datos_extraidos) <- c("FECHA", "HORA", "TEMP", "HUM", "PNM", "DD", "FF", "NOMBRE")
    datos_extraidos <- datos_extraidos %>% mutate(across(everything(), trimws))
    
    return(datos_extraidos) # Devolver datos como texto
    
  }, error = function(e) {
    warning(paste("Error GRAVE procesando con Regex:", ruta_archivo, "-", e$message))
    return(NULL)
  })
}
# -----------------------------------------------------------------
# PASO 5: Ejecutar el proceso y combinar todo
# -----------------------------------------------------------------

# 1. Obtener la lista de los archivos .txt
archivos <- list.files(path = ruta_carpeta,
                       pattern = "\\.txt$",
                       full.names = TRUE,
                       ignore.case = TRUE)

print(paste("Se encontraron", length(archivos), "archivos para importar."))

# 2. Aplicar la funci√≥n CORRECTA a todos los archivos
#    ¬°ASEG√öRATE de que el nombre aqu√≠ coincida con el de la funci√≥n!
dataset_temporal <- purrr::map_dfr(archivos,
                                   procesar_archivo_regex_general,
                                   .id = "archivo_origen")

print("Importaci√≥n (como texto) completada.")

# -----------------------------------------------------------------
# PASO 6: Convertir columnas al tipo de dato correcto
# -----------------------------------------------------------------
print("Convirtiendo tipos de datos...")

dataset_completo <- dataset_temporal %>%
  # Primero, reemplazar cadenas vac√≠as "" o que solo sean espacios con NA
  mutate(across(everything(), ~na_if(trimws(.), ""))) %>%
  mutate(
    # Convertir las columnas num√©ricas
    across(
      .cols = c(HORA, TEMP, HUM, PNM, DD, FF),
      .fns = as.numeric # Convierte a n√∫mero, los NA se mantienen
    ),
    
    # Convertir la FECHA
    FECHA = as.Date(FECHA, format = "%d%m%Y")
    
    # NOMBRE ya fue limpiado con trimws dentro de la funci√≥n y con na_if arriba
  )

print("¬°Conversi√≥n de datos completada!")

# -----------------------------------------------------------------
# PASO 7: Revisar el dataset final
# -----------------------------------------------------------------

print(paste("Total de filas importadas:", nrow(dataset_completo)))

print("Revisando las primeras 10 filas del dataset final:")
print(head(dataset_completo, 10))

print("Revisando la estructura final:")
str(dataset_completo)

print("Revisando un resumen estad√≠stico b√°sico:")
summary(dataset_completo)

# -----------------------------------------------------------------
# PASO 8: Guardar el dataset final üíæ
# -----------------------------------------------------------------
print("Guardando el dataset unificado en 'dataset_clima_unificado.Rds'...")

saveRDS(dataset_completo, file = "dataset_clima_unificado.Rds")

print("¬°Dataset guardado con √©xito! La pr√≥xima vez, solo usa readRDS('dataset_clima_unificado.Rds').")

readr::write_csv(dataset_completo, "dataset_clima_unificado.csv")
```

Primer checkpoint: Para no ejecutar todo lo de arriba
```{r}
dataset_completo <- readRDS("dataset_clima_unificado.Rds")
```

# 2. Limpieza de Datos y Tratamiento de NAs
Un primer resumen y conteo de valores NA revela que, aunque los datos est√°n le√≠dos, no est√°n limpios. M√∫ltiples columnas tienen una alta proporci√≥n de valores faltantes o an√≥malos que debemos tratar.
```{r}
dataset_met <- dataset_completo

summary(dataset_met)
# proporci√≥n de valores NA para estas columnas
attach(dataset_met) # adjuntando para evitar mencionar df$var cada vez

col1 = c() 
col2 = c()
col3 = c()

# bucle 
for (i in (1:dim(dataset_met)[2])){
  col1[i] = colnames(dataset_met)[i]
  col2[i] =  sum(is.na(dataset_met[i]))
  col3[i] = round(col2[i]/dim(dataset_met)[1],4)
}

# creando un dataframe para compilar los valores NA 
(df_NA <- tibble("Columna" = col1, "Valores_NA" = col2, "Proporcion_NA" = col3))

```

## 2.1. Limpieza de Temperatura (TEMP)

Comenzamos con la Temperatura. Un histograma nos muestra que la mayor√≠a de los datos se concentran en un rango l√≥gico, pero existen valores at√≠picos extremos (ej. > 50¬∞C o < -40¬∞C) que son f√≠sicamente imposibles.

Filtramos estos valores, convirti√©ndolos en NA.

```{r}
tibble(dataset_met)
summary(dataset_met)

ggplot(dataset_met, aes(x = TEMP)) +
  geom_histogram(binwidth = 0.5) +
  coord_cartesian(xlim = c(-100, 100), ylim = c(0, 1)) +
  labs(
    title = "La temperatura se concentra aproximadamente entre los -40 y 50 grados",
    subtitle = "Todos los dem√°s valores deben ser err√≥neos"
  )

# Ponemos NA donde la temperatura sea otra que ese intervalo

met_limpio <- dataset_met |>
  mutate(
    TEMP = case_when(
      TEMP > -40 & TEMP < 50 ~ TEMP #Si esta entre esos valores es la misma temperatura, sino pone NA
    )
  )
```

Esto incrementa nuestros NA en Temperatura, por lo que debemos imputarlos. Nuestra estrategia es iterativa y se basa en promedios m√≥viles:
- Primero, intentamos un promedio m√≥vil de 5 d√≠as (2 d√≠as antes, 2 despu√©s) agrupando por estaci√≥n y hora.
- Probamos una segunda imputaci√≥n con un promedio m√≥vil de 5 horas (2 horas antes, 2 despu√©s) agrupando por estaci√≥n y d√≠a.
- Esto reduce los NA a uno solo. Inspeccionamos este caso (en OBERA) y vemos que es un registro aislado para ese d√≠a, por lo que lo eliminamos.
```{r}
summary(met_limpio) # Ahora la minima es de -39.9 y la maxima de 45.60

# Pero no queremos que haya NA. ¬øC√≥mo podemos aproximar a la temperatura de ese d√≠a? Hacemos el promedio de dos horas atras y dos horas adelante.

met_limpio_1.2 <- met_limpio |>
  arrange(NOMBRE, FECHA) |> #Me doy cuenta que el NOMBRE en las primeras 120 filas son un n√∫mero (28) o una fecha, que ni siquiera corresponde a la fecha real. Quizas podriamos comparar con el dia anterior a ver que estaciones hay en una y no en la otra pero, al ser tan pocas, prefiero borrarlas.
  slice(-(1:120))

na_temp <- met_limpio_1.2 |>
  filter(is.na(TEMP)) |>
  group_by(FECHA, NOMBRE) |>
  summarise(n = n()) |>
  arrange(desc(n)) 
na_temp
# Hay mas de uno por d√≠a por estacion. Es m√°s, hay un dia en una estacion que tiene NA en 22 registros. Cambio de planes: Hacemos de dos dias atras y dos dias adelante en esa misma hora. En los valores que vuelva a haber NA hacemos dos horas atras y dos adelante

# --- Preparaci√≥n ---

met_iterativo <- met_limpio_1.2

# Obtenemos el conteo inicial de NAs
previous_na_count <- sum(is.na(met_iterativo$TEMP))

cat("Conteo inicial de NAs:", previous_na_count, "\n")

while (TRUE) {
  met_iterativo <- met_iterativo |>
    group_by(NOMBRE, HORA) |>
    arrange(FECHA, .by_group = TRUE) |>
    mutate(
    temp_promedio_movil = slide_dbl(
      TEMP, 
      ~mean(.x, na.rm = TRUE), 
      .before = 2, 
      .after = 2
    ),
    TEMP = if_else(is.na(TEMP), temp_promedio_movil, TEMP)
  ) |>
  ungroup() |>
    select(-temp_promedio_movil) 

  # 4. Calculamos el nuevo conteo de NAs
  new_na_count <- sum(is.na(met_iterativo$TEMP))
  
  cat("Iteraci√≥n completada. NAs restantes:", new_na_count, "\n")

  # 5. Condici√≥n de salida
  if (new_na_count < previous_na_count) {
    previous_na_count <- new_na_count
  } else {
    cat("Estabilizado. No hubo m√°s reducci√≥n de NAs. Saliendo del bucle.\n")
    break # ¬°Salimos del while!
  }
}

summary(met_iterativo) # 29 NAs restantes

# Probamos con otra l√≥gica. Promedio de una hora antes y una despu√©s

previous_na_count <- sum(is.na(met_iterativo$TEMP))

cat("Conteo inicial de NAs:", previous_na_count, "\n")

met_iterativo <- met_iterativo |>
    group_by(NOMBRE, FECHA) |>
    arrange(HORA, .by_group = TRUE) |>
    mutate(
    temp_promedio_movil = slide_dbl(
      TEMP, 
      ~mean(.x, na.rm = TRUE), 
      .before = 2, 
      .after = 2
    ),
    TEMP = if_else(is.na(TEMP), temp_promedio_movil, TEMP)
  ) |>
  ungroup() |>
    select(-temp_promedio_movil) 
  
new_na_count <- sum(is.na(met_iterativo$TEMP))
  
cat("Iteraci√≥n completada. NAs restantes:", new_na_count, "\n")
# Queda un solo NA, Lo visualizamos

met_iterativo |> 
  arrange(desc(is.na(TEMP)), FECHA)
# Es en OBERA, 2022-09-18

met_iterativo |>
  filter(NOMBRE == "OBERA",
         FECHA == "2022-09-18")
# Solo hay un registro de Obera ese dia. EN las otras columnas tambien tiene NA. Lo eliminamos.

met_limpio_1.4 <- met_iterativo |>
  arrange(desc(is.na(TEMP)), FECHA) |>
  slice(-1)
summary(met_limpio_1.4)
```

Tras limpiar la temperatura, guardamos el progreso.
```{r}
saveRDS(met_limpio_1.4, file = "met_limpio_1.4.Rds")
```

## 2.2. Limpieza de Humedad (HUM)

Corregimos exitosamente todos los NA de temperatura. Ahora analizamos la humedad.Un histograma muestra valores fuera del rango f√≠sico (0-100%). Los filtramos, convirti√©ndolos en NA.

```{r}
ggplot(met_limpio_1.4, aes(x = HUM)) +
  geom_histogram(binwidth = 0.5)
# Como es de esperar, la mayoria de valores estan entre el 0 y el 100. Como no puede haber mas de 100% d ehumedad los otros son NA.

met_limpio_2.1 <- met_limpio_1.4 |>
  mutate(
    HUM = case_when(
      HUM >= 0 & HUM <= 100 ~ HUM #Si esta entre esos valores es la misma temperatura, sino pone NA
    )
  )
summary(met_limpio_2.1)
```

Estimaremos los valores de humedad que no tenemos con una hora antes y una hora despu√©s, ya que es m√°s vol√°til que la temperatura.

Implementamos un bucle while que aplica un promedio m√≥vil de 3 horas (1 antes, 1 despu√©s) agrupado por estaci√≥n y d√≠a, repitiendo hasta que el n√∫mero de NA se estabilice.
```{r}
met_limpio_2.2 <- met_limpio_2.1
previous_na_count <- sum(is.na(met_limpio_2.2$HUM))

cat("Conteo inicial de NAs:", previous_na_count, "\n")

while (TRUE) {
  met_limpio_2.2 <- met_limpio_2.2 |>
    group_by(NOMBRE, FECHA) |>
    arrange(HORA, .by_group = TRUE) |>
    mutate(
    hum_promedio_movil = slide_dbl(
      HUM, 
      ~mean(.x, na.rm = TRUE), 
      .before = 1, 
      .after = 1
    ),
    HUM = if_else(is.na(HUM), hum_promedio_movil, HUM)
  ) |>
  ungroup() |>
    select(-hum_promedio_movil) 
  
  new_na_count <- sum(is.na(met_limpio_2.2$HUM))
  
  cat("Iteraci√≥n completada. NAs restantes:", new_na_count, "\n")

  if (new_na_count < previous_na_count) {
    previous_na_count <- new_na_count
  } else {
    cat("Estabilizado. No hubo m√°s reducci√≥n de NAs. Saliendo del bucle.\n")
    break # ¬°Salimos del while!
  }
}

summary(met_limpio_2.2)
ggplot(met_limpio_2.1, aes(x = HUM)) +
  geom_histogram(binwidth = 0.5)
ggplot(met_limpio_2.2, aes(x = HUM)) +
  geom_histogram(binwidth = 0.5)
```

Guardamos el progreso nuevamente.
```{r}
saveRDS(met_limpio_2.2, file = "met_limpio_2.2.Rds")
met_limpio_2.2 <- readRDS("met_limpio_2.2.Rds")
```

## 2.3. Limpieza de Presi√≥n (PNM)

Vemos la distribuci√≥n de Presi√≥n, que est√° medida en hPa.

El histograma de Presi√≥n (PNM) es m√°s complejo. Muestra la mayor√≠a de los valores en el rango l√≥gico (900-1050 hPa), pero tambi√©n c√∫mulos extra√±os cerca de 1500 y 3000 hPa.

Al investigar estos valores, descubrimos que se concentran en estaciones espec√≠ficas (ej. "SALTA AERO"). Una serie temporal de Salta revela que todos sus datos de presi√≥n son an√≥malos, sugiriendo un sensor defectuoso.

Decidimos filtrar todos los valores fuera del rango realista 900-1050 hPa, convirti√©ndolos en NA.
```{r}
met_limpio_3 <- met_limpio_2.2

met_limpio_3 |>
  filter(NOMBRE == "LA QUIACA OBSERVATORIO") |>
  view()
  

ggplot(met_limpio_3, aes(x = PNM)) +
  geom_histogram(binwidth = 0.5)
# Hay una especie de macha alrededor de los 1500hPa y poco m√°s de 3000. Ampliamos
ggplot(met_limpio_3, aes(x = PNM)) +
  geom_histogram(binwidth = 0.5) + 
  coord_cartesian(xlim = c(1375,1625))
ggplot(met_limpio_3, aes(x = PNM)) +
  geom_histogram(binwidth = 0.5) + 
  coord_cartesian(xlim = c(3000,3200))
# ¬øPor qu√© ser√°?

valores_raros <- met_limpio_3 |>
  filter(PNM > 1100 | PNM < 850) |>
  arrange(PNM)

valores_raros_count <- valores_raros |> 
  count(NOMBRE) |> 
  arrange(desc(n))
# Salta es la estacion con mas valores raros

presion_salta_diaria <- met_limpio_3 |>
  filter(NOMBRE == "SALTA AERO") |>
  group_by(FECHA) |>
  summarise(
    PRESION_media = mean(PNM, na.rm = TRUE)
  ) |>
  ungroup()

# Graficar la serie de tiempo
ggplot(presion_salta_diaria, aes(x = FECHA, y = PRESION_media)) +
  geom_line() +
  geom_point() + # Opcional: mostrar los puntos de cada d√≠a
  labs(
    title = "Evoluci√≥n de la Presi√≥n Promedio en Bariloche",
    y = "Presi√≥n Media Diaria (hPa)",
    x = "Fecha"
  ) +
  theme_minimal()

# Podemos ver que no hay un solo dato en Salta que est√© en lo par√°metros normales. Muy probablemente el sensor est√© roto.

met_limpio_3.1 <- met_limpio_3 |>
  mutate(
    PNM = if_else(
      PNM > 900 & PNM < 1050, PNM, NA_real_)
    )

```

Codigo borrado, pero en resumen: Se itero tantas veces que hubo sesgo de propagacion. La maxima cantidad de valores que se repetian por estacion era de 561 y pas√≥ a 26876.

Probamos otra estrategia. Usamos el paquete zoo para una imputaci√≥n m√°s controlada:
- na.approx: Interpolaci√≥n lineal para rellenar huecos peque√±os (m√°x. 2 horas).
- na.locf: "√öltima observaci√≥n llevada adelante" (LOCF) para huecos de hasta 2 horas.
- na.locf(fromLast = TRUE): "Pr√≥xima observaci√≥n tra√≠da hacia atr√°s" (NOCB) para los huecos restantes.

Este m√©todo rellena los NA de forma robusta sin distorsionar significativamente la distribuci√≥n original.
```{r}
met_relleno_final <- met_limpio_3.1 |>
  group_by(NOMBRE, FECHA) |>
  arrange(HORA, .by_group = TRUE) |>
  mutate(
    PNM_relleno_final = zoo::na.approx(PNM, maxgap = 2, na.rm = FALSE),
    PNM_relleno_final = zoo::na.locf(PNM_relleno_final, na.rm = FALSE, maxgap = 2),
    PNM_relleno_final = zoo::na.locf(PNM_relleno_final, fromLast = TRUE, na.rm = FALSE, maxgap = 2)
  ) |>
  ungroup()

nas_finales <- sum(is.na(met_relleno_final$PNM_relleno_final))
cat("NAs iniciales:", sum(is.na(met_limpio_3.1$PNM)), "\n")
cat("NAs finales restantes:", nas_finales, "\n")

summary(met_relleno_final)
ggplot(met_relleno_final, aes(x = PNM)) +
  geom_histogram(binwidth = 0.5)
ggplot(met_relleno_final, aes(x = PNM_relleno_final)) +
  geom_histogram(binwidth = 0.5) 
# El cambio en la distribucion es apenas perceptible. Me quedo con esta limpieza de NA
```

No aproximamos m√°s NAs para que no haya sesgo de propagaci√≥n. Formateamos y guardamos.

```{r}
met_limpio_3.2 <- met_relleno_final |>
  select(-PNM) |>
  rename(PNM = PNM_relleno_final)
saveRDS(met_limpio_3.2, "met_limpio_3.2.Rds")
met_limpio_3.2 <- readRDS("met_limpio_3.2.Rds")
```


# 3. Ingenier√≠a de Variables: Viento (DD y FF)

Ahora analizamos las variables de viento: Direcci√≥n (DD) y Velocidad (FF).
El histograma de Direcci√≥n (DD) muestra picos an√≥malos en 0 y 990.
- Encontramos que DD=0 casi siempre corresponde a FF=0 (Calma).
- DD=990 es un valor centinela conocido que significa "Variable".

Usamos esto para dos prop√≥sitos:
- Crear una nueva variable categ√≥rica DD_categoria (Norte, Noreste, Calmo, Variable).
- Limpiar la columna num√©rica DD: si FF=0, ponemos NA; si DD=990, ponemos NA.
```{r}
met_limpio_4 <- met_limpio_3.2
summary(met_limpio_4)

ggplot(met_limpio_4, aes(x = DD)) +
  geom_histogram(binwidth = 0.5)

DD_common <- met_limpio_4 |>
  group_by(DD) |>
  summarise(DD = mean(DD), n = n()) |>
  arrange(desc(DD))

# Primero: ¬øQu√© hacemos con grado = 0? ¬øEs lo mismo que 360?

met_limpio_4 |> 
  filter(DD == 0) |>
  count(FF) # Descubrimos que si el grado = 0 en casi todos los casos la velocidad tambien es 0. Decidimos poner NA en los casos donde FF = 0 porque en realidad nunca hubo direccion. Dejarlo en 0 rompera las estadisticas y los graficos. El otro gran pico anomalo esta en DD = 990. Esto es un valor sentinela en metereolog√≠a. Puede significar NA, calmo o variable.

met_limpio_4.1 <- met_limpio_4 |>
  mutate(
      DD_categoria = case_when(
      FF == 0 ~ "Calmo",
      DD == 990 ~ "Variable",
      is.na(FF) ~ NA_character_,
      (DD > 337.5 | DD <= 22.5) & FF > 0 ~ "Norte",
      DD > 22.5  & DD <= 67.5  ~ "Noreste",
      DD > 67.5  & DD <= 112.5 ~ "Este",
      DD > 112.5 & DD <= 157.5 ~ "Sureste",
      DD > 157.5 & DD <= 202.5 ~ "Sur",
      DD > 202.5 & DD <= 247.5 ~ "Suroeste",
      DD > 247.5 & DD <= 292.5 ~ "Oeste",
      DD > 292.5 & DD <= 337.5 ~ "Noroeste",
      .default = NA_character_
    ),
    DD = case_when(
      FF == 0 ~ NA_real_,
      DD == 0 & is.na(FF) ~ NA_real_,
      DD == 0 & FF > 0 ~ 360,
      DD > 360 ~ NA_real_,
      .default = DD
    ),
    DD_categoria = factor(DD_categoria)
  )
summary(met_limpio_4.1) 
```

Un histograma de Velocidad del Viento (FF) muestra algunos valores muy altos. Al investigarlos, vemos que provienen de bases ant√°rticas y son, de hecho, valores v√°lidos.
```{r}

ggplot(met_limpio_4.1) +
  geom_histogram(aes(x = FF))

met_limpio_4.1 |>
  filter(FF > 120) |>
  group_by(NOMBRE) |>
  summarize(n=n()) |>
  arrange(desc(n))
# No hay que filtrar nada. Los vientos fuertes son en bases antarticas
```

El pr√≥ximo paso es unir el dataset con las estaciones. Hay algunas estaciones cuyos nombres est√°n mal.

Detectamos que muchas estaciones tienen nombres ligeramente diferentes (ej. "LA QUIACA OBS." vs "LA QUIACA OBSERVATORIO"). Estandarizamos estos nombres usando case_when para asegurar que las uniones futuras funcionen correctamente.

```{r}
met_limpio_4.2 <- met_limpio_4.1 |>
  filter(NOMBRE != "}") |>
  mutate(
    NOMBRE = case_when(
      NOMBRE == "BUENOS AIRES" ~ "BUENOS AIRES OBSERVATORIO",
      NOMBRE == "ESC.AVIACION MILITAR AERO" ~ "ESCUELA DE AVIACION MILITAR AERO",
      NOMBRE == "ESCUELA DE AVIACION MILITA                                                                      R AERO" ~ "ESCUELA DE AVIACION MILITAR AERO",
      NOMBRE == "LA QUIACA OBS." ~ "LA QUIACA OBSERVATORIO",
      NOMBRE == "LAS FLORES AERO" ~ "LAS FLORES",
      NOMBRE == "OBERA AERO" ~ "OBERA",
      NOMBRE == "PCIA. ROQUE SAENZ PE√ëA AER O	" ~ "PRESIDENCIA ROQUE SAENZ PE√ëA AERO",
      NOMBRE == "PILAR OBS." ~ "PILAR OBSERVATORIO",
      NOMBRE == "PCIA. ROQUE SAENZ PE√ëA AER                                                                      O" ~ "PRESIDENCIA ROQUE SAENZ PE√ëA AERO",
      NOMBRE == "PRESIDENCIA ROQUE SAENZ PE                                                                      √ëA AERO" ~ "PRESIDENCIA ROQUE SAENZ PE√ëA AERO",
      NOMBRE == "SAN FERNANDO" ~ "SAN FERNANDO AERO",
      NOMBRE == "VENADO TUERTO" ~ "VENADO TUERTO AERO",
      NOMBRE == "VILLA DE MARIA DEL RIO SEC                                                                      O" ~ "VILLA DE MARIA DEL RIO SECO",
      NOMBRE == "VILLA MARIA DEL RIO SECO" ~ "VILLA DE MARIA DEL RIO SECO",
      .default = NOMBRE 
    )
  )

met_limpio_4.2 |>
  group_by(NOMBRE) |>
  summarise(n = n()) |>
  view()
```

Volvemos a analizar FF. Vamos a hacer una interpolacion para imputar NAs, con un gap muy corto de dos horas. Primero limpiamos un poco el DF (Volvemos a analizar FF. Vamos a hacer una interpolacion para imputar NAs, con un gap muy corto de dos horas.

Realizamos una pasada final de limpieza:
- Normalizamos los nombres de las columnas (ej. FECHA -> fecha) con janitor::clean_names.
- Creamos una columna datetime formal.
- Eliminamos registros duplicados (misma estaci√≥n, fecha y hora).
- Realizamos una imputaci√≥n final y conservadora sobre ff (Velocidad del Viento) usando zoo::na.approx y na.locf solo para huecos de 2 horas o menos. columnas, creando datetime y eliminando duplicados) para ya tenerlo listo para los pr√≥ximos analisis.
```{r}
met_limpio_4.3 <- met_limpio_4.2 |>
  clean_names() |> #Normalizamos los nombres para el siguiente paso, que va a ser unir los dataframes
  mutate(
    fecha        = as.Date(fecha),
    hora         = as.integer(hora),
    dd_categoria = as.factor(dd_categoria),
    pnm          = as.numeric(pnm),
    # Agregamos columna Datetime
    datetime = lubridate::make_datetime(
      year = lubridate::year(fecha),
      month = lubridate::month(fecha),
      day = lubridate::day(fecha),
      hour = hora,
      tz = "America/Argentina/Buenos_Aires"
    )
  ) |>
  # ====== Duplicados ====================================
  distinct(nombre, fecha, hora, .keep_all = TRUE) %>%

  # ========== Interpolaci√≥n intra-estaci√≥n de ff =============
  
  # 1. Ordenar los datos
  arrange(nombre, datetime) |>
  group_by(nombre) |>
  mutate(
    # Paso 1: Interpolaci√≥n lineal (zoo::na.approx)
    ff_interp = zoo::na.approx(ff,
                               x = as.numeric(datetime),
                               na.rm = FALSE, maxgap = 2),
    
    # Paso 2: Relleno hacia adelante (Last Observation Carried Forward). Se aplica sobre la columna 'ff_interp' reci√©n creada en el paso anterior.
    ff_interp = zoo::na.locf(ff_interp,
                             na.rm = FALSE, maxgap = 2),
    # Paso 3: Relleno hacia atr√°s (Next Observation Carried Backward). Se aplica sobre el resultado del Paso 2.
    ff_interp = zoo::na.locf(ff_interp,
                             fromLast = TRUE, na.rm = FALSE, maxgap = 2)
  ) |>
  ungroup() |>
  select(-ff) |>
  rename(ff = ff_interp)

met_datos_limpio <- met_limpio_4.3
```

Este met_datos_limpio es nuestro conjunto de datos horario, limpio y procesado. Lo guardamos.

```{r}
saveRDS(met_datos_limpio, "met_datos_limpio.Rds")
met_datos_limpio <- readRDS("met_datos_limpio.Rds")
```

# 4. Unificaci√≥n con Metadatos y Datos de Precipitaci√≥n

Ahora que nuestros datos observacionales est√°n limpios, los enriquecemos.
- Cargamos smn_estaciones.csv (que contiene latitud, longitud y provincia) y lo unimos.
- Cargamos un dataset separado de smn_precipitaciones-1991-2024.txt.
Unimos estos datos.
```{r}
smn_estaciones <- read_csv("smn_estaciones.csv")
smn_estaciones <- smn_estaciones |>
  clean_names()
met_con_info <- met_datos_limpio |>
  left_join(smn_estaciones, by = "nombre")

procesar_archivo_precipitacion <- function(ruta_archivo) {
  
  print(paste("Procesando (modo precipitaci√≥n):", ruta_archivo))
  
  tryCatch({
    
    # --- 1. Leer el CSV ---
    datos_precip <- readr::read_csv(
      ruta_archivo,
      locale = locale(encoding = "latin1"),
      show_col_types = FALSE # Oculta los mensajes de tipo de columna
    )
    
    # --- 2. Omitir archivos vac√≠os ---
    if (is.null(datos_precip) || nrow(datos_precip) == 0) {
      return(NULL)
    }
    
    # --- 3. Limpiar y renombrar columnas ---
    # El nombre de tu tercera columna "Precipitacion (mm) " es problem√°tico
    # (tiene un espacio o caracter raro al final).
    # La forma m√°s robusta de renombrarlo es por su posici√≥n.
    
    # Aqu√≠ seleccionamos las 3 columnas y les damos nombres limpios
    datos_limpios <- datos_precip |>
      select(
        Estacion = 1,
        Fecha = 2,
        Precipitacion_mm = 3
      )
      
    return(datos_limpios)
    
  }, error = function(e) {
    warning(paste("Error GRAVE procesando (modo precipitaci√≥n):", ruta_archivo, "-", e$message))
    return(NULL)
  })
}

datos_de_lluvia <- procesar_archivo_precipitacion("smn_precipitaciones-1991-2024/smn_precipitaciones-1991-2024.txt")

datos_de_lluvia <- datos_de_lluvia |>
  rename(
    nro = Estacion,
    fecha = Fecha,
    precipitacion = Precipitacion_mm
  )

df_final <- met_con_info |>
  left_join(
    datos_de_lluvia,
    by = c("nro", "fecha")
  )  
  

df_lluvia_con_info <- datos_de_lluvia |>
  left_join(smn_estaciones, by = "nro")
```

Guardamos los dataframes resultantes: df_final (datos horarios unidos con metadata y precipitaci√≥n diaria) y df_lluvia_con_info (datos de precipitaci√≥n diarios unidos con metadata).

```{r}
saveRDS(df_final, "df_final.Rds")
saveRDS(df_lluvia_con_info, "df_lluvia_con_info.Rds")


df_final <- readRDS("df_final.Rds")
df_lluvia_con_info <- readRDS("df_lluvia_con_info.Rds")
```

## 4.1. Desagregaci√≥n Temporal de Precipitaci√≥n

Hay un problema. El df de precipitaciones est√° por d√≠a, pero el de los datos est√° por hora.

Esto presenta un desaf√≠o clave: nuestro dataset principal es horario, pero la precipitaci√≥n es un total diario. Para analizarlos juntos, debemos desagregar ese total diario en estimaciones horarias.

Implementamos un sistema l√≥gico basado en la humedad:
- Si la precipitaci√≥n diaria es 0, todas las horas son 0.
- Plan A: Si hay horas con humedad >= 90%, repartimos la lluvia equitativamente solo entre esas horas.
- Plan B: Si no hay horas >= 90%, buscamos horas con humedad >= 70%. Repartimos la lluvia de forma ponderada (m√°s humedad, m√°s lluvia) entre esas horas.
- Plan C: Si todas las horas tienen < 70% de humedad, repartimos la lluvia equitativamente entre las 24 horas.

Esto crea la nueva columna precip_horaria.

```{r}
UMBRAL_ALTO <- 90
UMBRAL_BAJO <- 70

df_final_1.2 <- df_final |>
  group_by(nombre, fecha) |>
  mutate(
    # --- Conversi√≥n y Ayudas Plan A (Umbral 90%) ---
    precipitacion = as.numeric(precipitacion),
    es_hora_humeda_alta = (hum >= UMBRAL_ALTO & !is.na(hum)),
    n_horas_humedas_alta = sum(es_hora_humeda_alta),
    
    # --- Ayudas Plan B (Ponderado) ---
    hum_a_ponderar = if_else(hum >= UMBRAL_BAJO & !is.na(hum), hum, 0),
    total_hum_ponderada_dia = sum(hum_a_ponderar, na.rm = TRUE),

    # === El Case When Final ===
    precip_horaria = case_when(
      
      is.na(precipitacion) ~ NA_real_,
      precipitacion == 0 ~ 0,
      
      precipitacion > 0 & n_horas_humedas_alta > 0 & es_hora_humeda_alta == TRUE ~ precipitacion / n_horas_humedas_alta,
      precipitacion > 0 & n_horas_humedas_alta > 0 & es_hora_humeda_alta == FALSE ~ 0,
      
      precipitacion > 0 & n_horas_humedas_alta == 0 & total_hum_ponderada_dia > 0 ~ (hum_a_ponderar / total_hum_ponderada_dia) * precipitacion,
      
      precipitacion > 0 & n_horas_humedas_alta == 0 & total_hum_ponderada_dia == 0 ~ precipitacion / n(),
      
      .default = NA_real_ 
    )
  ) |>
  ungroup() |>
  select(
    -es_hora_humeda_alta, -n_horas_humedas_alta, 
    -hum_a_ponderar, -total_hum_ponderada_dia
  )|>
  arrange(fecha)

# Dice que hay 2189 advertencias con as.numeric(precipitacion)

df_final |>
  filter(is.na(as.numeric(precipitacion))) |>
  distinct(precipitacion) #son /N o NA
```

Guardamos este dataframe final, que ahora est√° completo y listo para el an√°lisis.

```{r}
saveRDS(df_final_1.2, "df_final_1.2.Rds")
df_final_1.2 <- readRDS("df_final_1.2.Rds")
```

# 5. Creaci√≥n de Datasets Agregados

Ahora creamos dataframes que nos ayudar√°n m√°s adelante para los gr√°ficos.

Para realizar un an√°lisis a nivel nacional, un simple promedio de todos los registros horarios ser√≠a incorrecto (sesgado). Debemos crear un promedio "justo".
- daily_station: Agregamos los datos horarios para obtener res√∫menes diarios por estaci√≥n (media de temp, suma de precip, etc.).
- monthly_by_station_fair: Agregamos los res√∫menes diarios a mensuales, por estaci√≥n. Filtramos los meses que no tengan al menos 25 d√≠as de datos (para que sean representativos).
- monthly_national_fair: Calculamos el promedio nacional "justo". Tomamos los promedios mensuales de cada estaci√≥n y calculamos la media de esos valores. As√≠, cada estaci√≥n tiene un solo "voto" por mes. Filtramos los meses nacionales que no tengan datos de al menos 60 estaciones.

```{r}
df <- df_final_1.2 

# ================== AGREGADOS PARA EDA ======================

# ---- Diario por estaci√≥n ----
daily_station <- df |>
  # Agrupamos por las columnas de la estaci√≥n y por d√≠a/a√±o/mes
  group_by(
    nro, nombre, provincia, latitud, longitud, 
    ymd = fecha,
    year = year(datetime),    
    month = month(datetime)   
  ) %>%
  # Calculamos los res√∫menes diarios
  summarise(
    registros  = n(),
    temp_mean  = if (all(is.na(temp))) NA_real_ else mean(temp, na.rm = TRUE),
    temp_sd    = sd(temp, na.rm = TRUE),
    hum_mean   = if (all(is.na(hum))) NA_real_ else mean(hum, na.rm = TRUE),
    pnm_mean   = if (all(is.na(pnm))) NA_real_ else mean(pnm, na.rm = TRUE),
    precip_sum = sum(precip_horaria, na.rm = TRUE),
    .groups = 'drop' 
  )

# ---- Nacional mensual "ingenuo" (para comparar) ----
monthly_national_raw <- df |>
  group_by(
    year = year(datetime),
    month = month(datetime)
  ) |>
  summarise(
    temp_mean = mean(temp, na.rm = TRUE),
    .groups = 'drop'
  ) |>
  mutate(
    ym = as.Date(paste0(year, "-", sprintf("%02d", month), "-01"))
  )

# ---- Nacional mensual "justo" (fair): diario->mensual por estaci√≥n->promedio nacional ----

monthly_by_station_fair <- daily_station |>
  group_by(
    nro, 
    ym = floor_date(ymd, "month") # redondea la fecha al primer d√≠a del mes
  ) |>
  summarise(
    y = mean(temp_mean, na.rm = TRUE),
    ndays = n(),
    .groups = 'drop'
  ) |>
  filter(ndays >= 25)

monthly_national_fair <- monthly_by_station_fair |>
  group_by(ym) |>
  # Calculamos el promedio de los promedios de las estaciones (y) y contamos cu√°ntas estaciones (n_est) pasaron el filtro anterior
  summarise(
    temp_mean = mean(y, na.rm = TRUE),
    n_est = n(),
    .groups = 'drop'
  ) |>
  filter(n_est >= 60) |>
  arrange(ym)


# --- Impresi√≥n del resumen de cobertura ---
cat("Cobertura media de estaciones por mes (fair):\n")
monthly_national_fair |>
  summarise(
    mean_n_est = mean(n_est),
    min_n_est = min(n_est),
    max_n_est = max(n_est)
  ) %>%
  print()
```

Ahora comparamos el m√©todo "raw" con el "fair".

El gr√°fico resultante muestra que ambas curvas son similares, pero el m√©todo "justo" maneja correctamente los datos incompletos al final del per√≠odo (Oct 2024), donde no promedia porque no cumple los filtros de calidad.

```{r}
# ---- PASO 1: Combinar los data frames ----
# Usamos bind_rows() para unirlos.
# .id = "metodo" crea una nueva columna llamada 'metodo' que toma el nombre que le dimos a cada dataframe ("Justo" o "Ingenuo").

df_comparativo <- bind_rows(
  Justo   = monthly_national_fair,
  Ingenuo = monthly_national_raw,
  .id = "metodo"
)

ggplot(df_comparativo, aes(x = ym, y = temp_mean, color = metodo)) +
  geom_line(linewidth = 1) +
  geom_point(alpha = 0.5) + 
  labs(
    title = "Comparaci√≥n de Promedio Mensual de Temperatura Nacional",
    subtitle = "M√©todo 'Justo' (promedio de estaciones) vs. 'Ingenuo' (promedio de lecturas)",
    x = "Fecha",
    y = "Temperatura Media (¬∞C)",
    color = "M√©todo de C√°lculo"
  ) +
  theme_minimal() +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") 
```

Guardamos estos importantes dataframes agregados.

```{r}
saveRDS(df, "df.Rds")
saveRDS(daily_station, "daily_station.Rds")
saveRDS(monthly_by_station_fair, "monthly_by_station_fair.Rds")
saveRDS(monthly_national_fair, "monthly_national_fair.Rds")
```


# 6. An√°lisis Visual Exploratorio

Con los datos limpios y agregados, comenzamos la exploraci√≥n visual.

## Bloque 1: Conociendo el Conjunto de Datos

Primero, una visi√≥n general de la cobertura de datos.

```{r}
# =========== INTRO EDUCATIVA (P√öBLICO B√ÅSICO) ============= 
# ---- 1 fila por estaci√≥n (para mapa y barras) ----
stations_tbl <- daily_station %>%
  filter(!is.na(latitud) & !is.na(longitud)) %>%
  group_by(nro) %>%
  slice(1) %>%
  ungroup() %>%
  select(nro, nombre, provincia, latitud, longitud)
```

Un mapa interactivo nos permite explorar la ubicaci√≥n de las estaciones, coloreadas por provincia.
```{r}
# ========= BLOQUE 1 ‚Äì Conociendo el conjunto de datos ==========
provincias_geo <- sf::st_read("shiny_app/data/provincias.geojson")
# Usamos "Set3" o "Paired" que tienen muchos colores distintos.
pal <- colorFactor(
  palette = "Set3", # Una paleta con colores variados (como en tu mapa)
  domain = provincias_geo$nombre # Basamos los colores en los nombres de provincia
)

m_intro <- leaflet() %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  
  # CAPA 1: Pol√≠gonos de Provincias (AHORA NO INTERACTIVOS)
  addPolygons(
    data = provincias_geo,
    fillColor = ~pal(nombre), 
    weight = 1,
    opacity = 1,
    color = "white",
    fillOpacity = 0.7,
    
    # --- CAMBIO CLAVE (Sintaxis correcta) ---
    # Desactivamos el clic y el resaltado
    options = pathOptions(interactive = FALSE) 
  ) %>%
  
  # CAPA 2: Marcadores de Estaciones (ENCIMA Y CLICABLES)
  addCircleMarkers(
    data = stations_tbl,
    lng = ~longitud, 
    lat = ~latitud,
    radius = 4, 
    stroke = TRUE, 
    weight = 1,
    color = "black",
    fillColor = "white",
    fillOpacity = 0.9,
    
    # El popup de la estaci√≥n ahora funcionar√°
    popup = ~sprintf("<b>%s</b><br/>Provincia: %s", nombre, provincia)
  ) %>%
  
  setView(lng = -64, lat = -38.4, zoom = 4)

m_intro
```

Un gr√°fico de barras cuantifica esto, mostrando el n√∫mero de estaciones por provincia.
```{r}
# (2) Barras: cantidad de estaciones por provincia
bars_df <- stations_tbl %>%
  count(provincia, name = "estaciones") %>%
  arrange(desc(estaciones))

# --- Gr√°fico ggplot modificado ---
p_bars <- ggplot(bars_df, aes(x = reorder(provincia, estaciones), 
                             y = estaciones, 
                             fill = estaciones,
                             # --- A√ëADE ESTA L√çNEA ---
                             text = paste("Estaciones:", estaciones) 
                             )) +
  geom_col() +
  scale_fill_distiller(palette = "Blues", direction = 1) + 
  coord_flip() +
  labs(x = NULL, y = "Cantidad de estaciones", title = "Estaciones por provincia") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "none")

# Convertir a plotly
p_bars <- ggplotly(p_bars, tooltip = "text")
p_bars
```

Finalmente, una serie temporal muestra el n√∫mero de estaciones activas (que reportan datos) por a√±o. Se observa una notable ca√≠da en los a√±os m√°s recientes del conjunto de datos.
```{r}
# (3) Timeline: estaciones activas por a√±o (simple)
# (Tu c√≥digo de preparaci√≥n de datos)
years_active <- daily_station %>%
  group_by(year) %>%
  summarise(estaciones_activas = n_distinct(nombre), .groups = "drop") %>%
  arrange(year)

# --- C√°lculo din√°mico de los l√≠mites del zoom ---
min_val <- min(years_active$estaciones_activas, na.rm = TRUE)
max_val <- max(years_active$estaciones_activas, na.rm = TRUE)
padding <- (max_val - min_val) * 0.2

p_timeline_zoom <- ggplot(years_active, aes(year, estaciones_activas)) +
  geom_area(fill = "#d62728", alpha = 0.3) + 
  geom_line(color = "#d62728", linewidth = 1.2) +
  
  scale_x_continuous(breaks = pretty_breaks()) +
  
  labs(
    x = "A√±o", y = "Estaciones activas", 
    title = "Ca√≠da Reciente en el Conteo de Estaciones Activas",
    subtitle = "Nota: El eje Y (vertical) no inicia en 0 para enfatizar la variaci√≥n."
  ) +
  
  # --- LA PARTE CLAVE ---
  # Hacemos zoom en el eje Y. Los l√≠mites se calculan din√°micamente.
  coord_cartesian(ylim = c(min_val - padding, max_val + padding)) +
  
  theme_minimal(base_size = 13)

p_timeline_zoom <- ggplotly(p_timeline_zoom, tooltip = c("x", "y"))
p_timeline_zoom
```

## Bloque 2: ¬øQu√© variables se registraron?

Una tabla simple de KPIs (Indicadores Clave de Rendimiento) nos da los promedios generales del dataset.
```{r}
# ======== BLOQUE 2 ‚Äì ¬øQu√© variables se registraron? =========

# (4) KPIs sencillos (tabla) ‚Äî se imprime en el Rmd
kpis <- daily_station %>%
  summarise(
    # Envuelve los nombres con espacios y caracteres especiales en `backticks`
    `Temp_media (¬∞C)`          = mean(temp_mean, na.rm = TRUE),
    `Humedad_media (%)`        = mean(hum_mean, na.rm = TRUE),
    `Presi√≥n_media (hPa)`      = mean(pnm_mean, na.rm = TRUE),
    `Lluvia_diaria_media (mm)` = mean(precip_sum, na.rm = TRUE)
  )

kpis_table <- knitr::kable(kpis, digits = 1, align = "r")
kpis_table
```

Luego, generamos histogramas para visualizar la distribuci√≥n de las principales variables diarias: Temperatura, Humedad y Presi√≥n.
```{r}
# (5) Histogramas simples por variable
mk_hist <- function(df, var, titulo, xlab, 
                    bins = 40, 
                    fill_color = "#1f77b4", # <-- 1. Nuevo argumento con un color azul
                    alpha = 0.8) {          # <-- 2. Nuevo argumento para transparencia
  
  gg <- ggplot(df, aes(x = .data[[var]])) +
    geom_histogram(
      bins = bins, 
      boundary = 0, 
      color = "white",      # Mantiene el borde blanco
      fill = fill_color,    # <-- 3. Usa el color de relleno
      alpha = alpha         # <-- 4. Aplica la transparencia
    ) +
    labs(title = titulo, x = xlab, y = "Frecuencia") +
    theme_minimal(base_size = 13) +
    theme(legend.position = "none") # Oculta la leyenda (no necesaria)
  
  ggplotly(gg, tooltip = c("x","y"))
}
```

```{r}
dist_temp <- mk_hist(daily_station, "temp_mean",  "¬øC√≥mo se reparte la temperatura?", "Temperatura (¬∞C)")
dist_temp

dist_hum <- mk_hist(daily_station, "hum_mean",   "¬øC√≥mo se reparte la humedad?", "Humedad relativa (%)")
dist_hum

dist_pnm <- mk_hist(daily_station, "pnm_mean",   "¬øC√≥mo se reparte la presi√≥n?", "Presi√≥n (hPa)")
dist_pnm
```

## Bloque 3: Una mirada r√°pida a los patrones

¬øC√≥mo interact√∫an estas variables?

Este gr√°fico muestra el ciclo estacional promedio de temperatura. Los puntos est√°n coloreados para mostrar claramente los meses fr√≠os (azul) y c√°lidos (rojo).
```{r}
# ===================== BLOQUE 3 ‚Äì Una mirada r√°pida a los patrones =====================

# (6) Serie mensual promedio nacional (muy simple)
# (1) Preparaci√≥n de datos (igual que tu c√≥digo)
monthly_mean <- daily_station %>%
  group_by(month) %>%
  summarise(temp = mean(temp_mean, na.rm = TRUE), .groups = "drop") %>%
  arrange(month) %>%
  mutate(mes = factor(month.abb[month], levels = month.abb))

# (2) Calcular el punto medio de la temperatura para centrar la paleta
mid_temp <- mean(monthly_mean$temp, na.rm = TRUE)

# --- (3) Gr√°fico ggplot Mejorado ---
p_month <- ggplot(monthly_mean, aes(x = mes, y = temp, group = 1,
                                   text = paste0("Mes: ", mes, "<br>Temp: ", 
                                                 sprintf("%.1f", temp), "¬∞C")
                                   )) +
  
  # --- CAMBIO AQU√ç ---
  # 1. La l√≠nea ahora es de un color gris est√°tico.
  geom_line(color = "grey80", linewidth = 1.2) + 
  
  # 2. Los puntos S√ç llevan el color din√°mico.
  geom_point(aes(color = temp), size = 3) +
  # --- FIN DEL CAMBIO ---
  
  scale_color_gradient2(
    low = "dodgerblue4",  
    mid = "white",        
    high = "firebrick",     
    midpoint = mid_temp   
  ) +
  
  labs(x = "Mes", y = "Temperatura media (¬∞C)", 
       title = "Ciclo Estacional de Temperatura") +
  theme_minimal(base_size = 13) +
  theme(legend.position = "none")

# --- (4) ggplotly con tooltip personalizado ---
p_month <- ggplotly(p_month, tooltip = "text")
p_month
```

Usamos un gr√°fico de hex√°gonos (geom_hex) para visualizar la densidad de la relaci√≥n entre temperatura y humedad. Las √°reas m√°s oscuras indican las condiciones m√°s frecuentes. La l√≠nea de regresi√≥n (roja) confirma la correlaci√≥n negativa: a m√°s calor, menos humedad relativa.
```{r}
# (7) Relaci√≥n simple: Temperatura vs Humedad (muestra para no sobrecargar)
set.seed(123)
d_samp <- daily_station %>%
  select(temp_mean, hum_mean) %>%
  tidyr::drop_na()
if (nrow(d_samp) > 3000) d_samp <- d_samp %>% dplyr::slice_sample(n = 3000)

# --- Gr√°fico con geom_hex ---
p_sc_hex <- ggplot(d_samp, aes(x = temp_mean, y = hum_mean)) +
  
  # 1. Reemplazamos geom_point por geom_hex
  geom_hex(bins = 50) + # 'bins' controla el tama√±o de los hex√°gonos
  
  # 2. A√±adimos una escala de color (fill)
  # 'trans = "log10"' es clave: evita que los puntos m√°s densos "saturen" la escala
  scale_fill_gradient(
    low = "lightblue", 
    high = "navyblue", 
    trans = "log10", 
    name = "Conteos" # T√≠tulo de la leyenda
  ) +
  
  # 3. Mantenemos la l√≠nea de regresi√≥n (en rojo para que resalte)
  geom_smooth(method = "lm", se = FALSE, color = "red", linewidth = 1) +
  
  labs(x = "Temperatura (¬∞C)", y = "Humedad relativa (%)",
       title = "Densidad de la Relaci√≥n Temperatura-Humedad") +
  theme_minimal(base_size = 13)

# 4. ggplotly (sin 'tooltip') para que muestre el conteo autom√°ticamente
p_sc_hex <- ggplotly(p_sc_hex)
p_sc_hex
```

Ac√° usamos los datos del df original (horarios) para un an√°lisis de densidad m√°s profundo.

Un gr√°fico de contorno 2D confirma la relaci√≥n inversa entre temperatura y humedad.
```{r}
# ==================== CORRELACIONES ==========================
set.seed(123)
df_smpl <- df %>%
  slice_sample(n = 300000)

p_th_contour <- plot_ly(df_smpl, x = ~temp, y = ~hum, 
                        type = "histogram2dcontour",
                        colorscale = "Hot",
                        colorbar = list(title = "<b>Nro. de<br>Observaciones</b>")
                       ) %>%
  layout(
    # --- MODIFICACI√ìN 2: T√≠tulos m√°s descriptivos ---
    title = list(text = "Relaci√≥n entre temperatura y humedad relativa<br><sup>A mayor temperatura, tiende a haber menor humedad. Densidad 2D de 300k obs.</sup>"),
    
    xaxis = list(title = "Temperatura (¬∞C)"),
    yaxis = list(title = "Humedad relativa (%)"),
    
    # --- MODIFICACI√ìN 3: Arreglar el margen superior ---
    margin = list(
        b = 80, # Margen inferior para la "Fuente"
        t = 80  # Nuevo margen superior para que entre el t√≠tulo/subt√≠tulo
    ),
    
    annotations = list(
      list(text = "Fuente: SMN ‚Äì elaboraci√≥n propia.",
           showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
           xanchor = 'right', yanchor = 'auto', font = list(size = 10))
    )
  )

p_th_contour
```

Lo mismo pero entre temperatura y presi√≥n.

Este histograma 2D muestra que las temperaturas m√°s altas tienden a coincidir con presiones atmosf√©ricas ligeramente menores, con una gran concentraci√≥n de datos entre 1008 y 1022 hPa.
```{r}
# ---- Histograma de Densidad: TEMP vs PNM ----

p_tp <- plot_ly(df_smpl, x = ~temp, y = ~pnm, type = "histogram2d") %>%
  
  # Usar plotly::layout para agrupar TODA la configuraci√≥n
  plotly::layout(
    
    title = list(text = "Temperaturas m√°s altas tienden a coincidir con presiones atmosf√©ricas ligeramente menores<br><sup>Densidad 2D con concentraci√≥n en el rango 1008‚Äì1022 hPa</sup>"),
    xaxis = list(title = "Temperatura (¬∞C)"),
    yaxis = list(title = "Presi√≥n a Nivel del Mar (hPa)"),
    
    # --- MODIFICACI√ìN: 'colorbar' ahora vive DENTRO de layout ---
    colorbar = list(title = "<b>Densidad de Horas</b>"), # Le agregu√© negrita
    
    # El margen que soluciona el corte del t√≠tulo
    margin = list(b = 80, t = 100), 
    
    annotations = list(
      list(text = "Fuente: SMN ‚Äì elaboraci√≥n propia.",
           showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
           xanchor = 'right', yanchor = 'auto', font = list(size = 10))
    )
  )

# Dejar que el notebook renderice el objeto
p_tp
```

## Bloque 4: An√°lisis Geoespacial (Coropletas)

Creamos un mapa de coropletas (mapa de pol√≠gonos coloreados) para visualizar la temperatura media por provincia, revelando el claro gradiente t√©rmico de norte a sur.
```{r}

# ======= (Opcional) BLOQUE 4 ‚Äì Mapa simple por provincia ========

station_clima <- daily_station %>%
    group_by(nro, provincia) %>%
    summarise(temp_base = mean(temp_mean, na.rm = TRUE), .groups = "drop")

prov_temp <- station_clima %>%
    group_by(provincia) %>%
    summarise(temp_prov = mean(temp_base, na.rm = TRUE), .groups = "drop") %>%
    mutate(
      provincia_join_key = toupper(iconv(provincia, from = "UTF-8", to = "ASCII//TRANSLIT")),
      provincia_join_key = dplyr::case_when(
        provincia_join_key == "CAPITAL FEDERAL" ~ "CABA",
        provincia_join_key == "TIERRA DEL FUEGO" ~ "TIERRA DEL FUEGO",
        TRUE ~ provincia_join_key
      )
    )
provincias_geo <- provincias_geo %>%
  mutate(
    provincia_join_key = toupper(iconv(nombre, from = "UTF-8", to = "ASCII//TRANSLIT")),
    provincia_join_key = dplyr::case_when(
      provincia_join_key == "CAPITAL FEDERAL" ~ "CABA",
      provincia_join_key == "CIUDAD AUTONOMA DE BUENOS AIRES" ~ "CABA",
      provincia_join_key == "TIERRA DEL FUEGO, ANTARTIDA E ISLAS DEL ATLANTICO SUR" ~ "TIERRA DEL FUEGO",
      TRUE ~ provincia_join_key
    )
  )

map_df <- provincias_geo %>%
    left_join(prov_temp, by = c("provincia_join_key"))

  pal <- colorNumeric("RdYlBu", domain = map_df$temp_prov, reverse = TRUE, na.color = "#BDBDBD")

m_choro <- leaflet(map_df) |>
    addProviderTiles(providers$CartoDB.Positron) |>
    addPolygons(
      fillColor = ~pal(temp_prov), color = "white", weight = 1, opacity = 1,
      fillOpacity = 0.7,
      label = ~htmltools::HTML(sprintf("<b>%s</b><br/>Temp media: %s ¬∞C",
                                       nombre, ifelse(is.na(temp_prov), "s/d", sprintf("%.1f", temp_prov))))
    ) |>
    addLegend("bottomright", pal = pal, values = ~temp_prov,
              title = "Temp media (¬∞C)", opacity = 1) |>
    setView(lng = -64, lat = -38.4, zoom = 4)
m_choro


# ===================== FIN INTRO EDUCATIVA (dplyr, sin saves) =====================
```

A continuaci√≥n, creamos un mapa interactivo multicapa mucho m√°s avanzado. Este mapa permite al usuario superponer y alternar entre cuatro vistas diferentes:
- Estaciones (Temp media): Puntos coloreados por temperatura media.
- Estaciones (Amplitud): Puntos coloreados por amplitud t√©rmica (diferencia P90-P10).
- Provincias (Temp media): El mapa de coropletas que acabamos de ver.
- Provincias (Amplitud): Un mapa de coropletas de la amplitud t√©rmica.
Esto proporciona una herramienta muy rica para explorar patrones geoespaciales.

```{r}
# --- 1. PREPARACI√ìN DE DATOS (Tu c√≥digo original) ----

# (Extremos)
ext <- df %>%
  filter(!is.na(temp)) %>%
  group_by(nro, nombre, provincia, latitud, longitud) %>%
  summarise(
    p10 = quantile(temp, .10, na.rm = TRUE),
    p90 = quantile(temp, .90, na.rm = TRUE),
    amp = p90 - p10, 
    .groups = 'drop' 
  )

# (Resumen de estaciones)
stations <- df %>%
  group_by(nro, nombre, provincia) %>%
  summarise(
    lat = first(latitud),
    lon = first(longitud),
    temp_mean = mean(temp, na.rm = TRUE),
    sd_temp = sd(temp, na.rm = TRUE),
    registros = n(),
    .groups = 'drop'
  ) %>%
  filter(!is.na(lat) & !is.na(lon) & !is.na(temp_mean))

# ---- 2. PRE-C√ÅLCULO DEL RADIO (Tu c√≥digo original) ----
stations <- stations %>%
  mutate(
    radius_val = scales::rescale(registros, to = c(3, 12))
  )
ext <- ext %>%
  left_join(
    stations %>% select(nro, registros, radius_val),
    by = "nro"
  )

# --- 3. CARGAR GEOJSON Y AGREGAR DATOS (NUEVO) ---

# 3.1 Funciones de ayuda (CORREGIDA)
normalize_name <- function(name_vector) {
  
  # 1. Normalizaci√≥n (sin acentos, may√∫sculas)
  key <- toupper(iconv(name_vector, from = "UTF-8", to = "ASCII//TRANSLIT"))
  
  # 2. Limpieza (aplicar case_when al vector 'key')
  dplyr::case_when(
    key == "CAPITAL FEDERAL" ~ "CABA",
    key == "CIUDAD AUTONOMA DE BUENOS AIRES" ~ "CABA",
    key == "TIERRA DEL FUEGO, ANTARTIDA E ISLAS DEL ATLANTICO SUR" ~ "TIERRA DEL FUEGO",
    # (Mantenemos el valor por defecto 'key')
    TRUE ~ key 
  )
}

# 3.2 Cargar y preparar el GeoJSON (Ahora funcionar√°)
provincias_sf <- provincias_geo %>%
  mutate(provincia_join_key = normalize_name(nombre))

# 3.3 Agregar datos de ESTACIONES a nivel PROVINCIA
prov_temp <- stations %>%
  mutate(provincia_join_key = normalize_name(provincia)) %>%
  group_by(provincia_join_key) %>%
  summarise(
    temp_prov = mean(temp_mean, na.rm = TRUE),
    registros_prov = sum(registros, na.rm = TRUE)
  )

# 3.4 Agregar datos de AMPLITUD a nivel PROVINCIA
prov_amp <- ext %>%
  mutate(provincia_join_key = normalize_name(provincia)) %>%
  group_by(provincia_join_key) %>%
  summarise(
    amp_prov = mean(amp, na.rm = TRUE)
  )

# 3.5 Unir datos agregados al GeoJSON
map_df_temp <- provincias_sf %>%
  left_join(prov_temp, by = "provincia_join_key")
  
map_df_amp <- provincias_sf %>%
  left_join(prov_amp, by = "provincia_join_key")

# ---- 4. PALETAS Y T√çTULO (Tu c√≥digo original) ----
pal_mean <- colorNumeric(
  palette = c("#0000FF", "#FFFFFF", "#FF0000"),
  domain = stations$temp_mean, 
  na.color = NA
)
pal_amp  <- colorNumeric(viridisLite::magma(256), domain = ext$amp, na.color = NA)

title_html <- '<div style="background-color: rgba(255,255,255,0.8); 
                      border: 1px solid grey; border-radius: 5px; 
                      padding: 5px 10px; text-align: center;">
              <h4 style="margin:0;">An√°lisis de Estaciones SMN</h4>
              <span style="font-size: 12px;">Temp. Media vs. Amplitud T√©rmica</span>
              </div>'

# ---- 5. CREACI√ìN DEL MAPA (Modificado) ----
m_map <- leaflet() %>% 
  addTiles() %>%
  addControl(html = title_html, position = "topleft") %>%
  
  # --- Capa 1: Estaciones (Temp media) ---
  addCircleMarkers(
    data = stations,
    lng = ~lon, lat = ~lat, 
    group = "Estaciones (Temp media)", # <-- Grupo 1
    radius = ~radius_val,
    color = ~pal_mean(temp_mean), fillColor = ~pal_mean(temp_mean),
    fillOpacity = 0.85, stroke = FALSE,
    popup = ~paste0("<b>", nombre, "</b><br>", provincia,
                    "<br>Registros: ", registros,
                    "<br>Temp media: ", round(temp_mean,2), " ¬∞C",
                    "<br>SD: ", round(sd_temp,1), " ¬∞C")
  ) %>%
  addLegend(position="bottomright", pal=pal_mean, values=stations$temp_mean,
            title="Temp media (¬∞C)", group="Estaciones (Temp media)") %>%
  
  # --- Capa 2: Estaciones (Amplitud) ---
  addCircleMarkers(
    data = ext,
    lng = ~longitud, lat = ~latitud, 
    group = "Estaciones (Amplitud)", # <-- Grupo 2
    radius = ~radius_val,
    color = ~pal_amp(amp), fillColor = ~pal_amp(amp),
    fillOpacity = 0.85, stroke = FALSE,
    popup = ~paste0("<b>", nombre, "</b><br>", provincia,
                    "<br>Registros: ", registros,
                    "<br>Amplitud t√©rmica (p90-p10): ", round(amp,1), " ¬∞C")
  ) %>%
  addLegend(position="bottomleft", pal=pal_amp, values=ext$amp,
            title="Amplitud (¬∞C)", group="Estaciones (Amplitud)") %>%

  # --- Capa 3: Provincias (Temp media) (NUEVO) ---
  addPolygons(
    data = map_df_temp,
    group = "Provincias (Temp media)", # <-- Grupo 3
    fillColor = ~pal_mean(temp_prov),
    weight = 1, opacity = 1, color = "white", fillOpacity = 0.7,
    highlightOptions = highlightOptions(weight = 3, color = "#666", fillOpacity = 0.9, bringToFront = TRUE),
    label = ~sprintf("<b>%s</b><br/>Temp media prov.: %s ¬∞C", 
                     nombre, round(temp_prov, 1))
  ) %>%
  addLegend(position="bottomright", pal=pal_mean, values=stations$temp_mean, # Reusa la paleta
            title="Temp media prov. (¬∞C)", group="Provincias (Temp media)") %>%

  # --- Capa 4: Provincias (Amplitud) (NUEVO) ---
  addPolygons(
    data = map_df_amp,
    group = "Provincias (Amplitud)", # <-- Grupo 4
    fillColor = ~pal_amp(amp_prov),
    weight = 1, opacity = 1, color = "white", fillOpacity = 0.7,
    highlightOptions = highlightOptions(weight = 3, color = "#666", fillOpacity = 0.9, bringToFront = TRUE),
    label = ~sprintf("<b>%s</b><br/>Amplitud prov.: %s ¬∞C", 
                     nombre, round(amp_prov, 1))
  ) %>%
  addLegend(position="bottomleft", pal=pal_amp, values=ext$amp, # Reusa la paleta
            title="Amplitud prov. (¬∞C)", group="Provincias (Amplitud)") %>%

  # --- Control de capas (Ahora con 4 grupos) ---
  addLayersControl(
    baseGroups = c("Estaciones (Temp media)", "Provincias (Temp media)", 
                   "Estaciones (Amplitud)", "Provincias (Amplitud)"), 
    options = layersControlOptions(collapsed = FALSE, autoZIndex = TRUE)
  )
  
# ---- 6. MOSTRAR EL MAPA ----
m_map
```

# 7. An√°lisis Profundo de Series Temporales

Volvemos a nuestro dataframe "justo" (monthly_national_fair) para un an√°lisis m√°s profundo de las tendencias a lo largo del tiempo.

Un histograma de las temperaturas horarias (usando una muestra) nos permite anotar la media y mediana exactas.

```{r}
# =========================== VISUALES ===========================

# ---- Histograma de temperatura (muestra) con media/mediana y l√≠mites √∫tiles
set.seed(123)
mu       <- mean(df$temp); md <- median(df$temp)
p_hist <- plot_ly(x = sample(df$temp, 200000), type = "histogram", nbinsx = 100) %>%
  plotly::layout(
    title = list(
      text = "La distribuci√≥n de temperaturas horarias muestra un amplio rango estacional<br><sup>Muestra de hasta 200k registros; l√≠neas punteadas = media y mediana</sup>",
      x = 0.05,
      xanchor = 'left'
    ),
    xaxis = list(title = "Temperatura (¬∞C)"),
    yaxis = list(title = "Frecuencia (conteo de horas)"),
    shapes = list(
      list(type="line", x0=mu, x1=mu, y0=0, y1=1, xref="x", yref="paper", line=list(dash="dash")),
      list(type="line", x0=md, x1=md, y0=0, y1=1, xref="x", yref="paper", line=list(dash="dot"))
    ),
    annotations = list(
      # --- Anotaci√≥n para la Media (mu = 16.3) ---
      list(
        x = mu, y = 0.88,  # <-- Posici√≥n Y
        xref = "x", yref = "paper",
        text = paste0("Media: ", round(mu,1),"¬∞C"),
        xanchor = 'right', # <-- Mueve el texto a la izquierda del punto
        showarrow = TRUE, 
        ax = -40, # <-- Flecha apunta desde la izquierda (-40)
        ay = -40,
        font = list(size = 10)
      ),
      # --- Anotaci√≥n para la Mediana (md = 16.8) ---
      list(
        x = md, y = 0.82, # <-- Posici√≥n Y (m√°s baja)
        xref = "x", yref = "paper",
        text = paste0("Mediana: ", round(md,1),"¬∞C"),
        xanchor = 'left', # <-- Mueve el texto a la derecha del punto
        showarrow = TRUE, 
        ax = 40, # <-- Flecha apunta desde la derecha (40)
        ay = -30,
        font = list(size = 10)
      ),
      # --- Anotaci√≥n para la fuente (footer) ---
      list(text = "Fuente: Servicio Meteorol√≥gico Nacional (SMN) ‚Äì elaboraci√≥n propia.",
           showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
           xanchor = 'right', yanchor = 'auto', font = list(size = 10))
    ),
    margin = list(t = 100, b = 80)
  )

p_hist
```

Trazamos la serie temporal nacional "justa" y le ajustamos una tendencia lineal. El gr√°fico muestra un ciclo estacional muy fuerte. Aunque la l√≠nea de tendencia parece tener una ligera pendiente negativa, el p-valor (p > 0.05) es alto. Esto indica que la tendencia observada no es estad√≠sticamente significativa y es consistente con el azar. A efectos pr√°cticos, la tendencia en este per√≠odo es plana.

```{r}
# 1. Preparar la l√≠nea de tendencia (ajustar el modelo lineal)
df_trend <- monthly_national_fair %>% 
  select(ym, temp_mean)

mdl <- lm(temp_mean ~ as.numeric(ym), data = df_trend)

# 2. Calcular los valores predichos
df_trend <- df_trend %>% 
  mutate(predicted = predict(mdl))

# --- INICIO DE LA CORRECCI√ìN ---
# 2.1. Extraer los resultados del modelo
model_summary <- summary(mdl)

# 2.2. Obtener la pendiente (slope) y el p-valor (pv)
# El coeficiente [2,1] es la pendiente (estimaci√≥n de 'as.numeric(ym)')
# El predictor 'as.numeric(ym)' cuenta en d√≠as, as√≠ que convertimos la pendiente a d√©cadas
slope_daily <- model_summary$coefficients[2, 1]
slope_decade <- slope_daily * 365.25 * 10 # Convertir de d√≠as a d√©cadas

# El coeficiente [2,4] es el p-valor de la pendiente
pv <- model_summary$coefficients[2, 4]

p_trend <- plot_ly(df_trend, x = ~ym) %>%
  
  # Capa 1: Datos observados (Serie de tiempo)
  add_trace(y = ~temp_mean, name = 'Temperatura Observada (Media)', 
            type = 'scatter', mode = 'lines+markers', 
            line = list(color = '#1f77b4'), marker = list(size = 4)) %>%
  
  # Capa 2: L√≠nea de Tendencia (Modelo Lineal)
  add_trace(y = ~predicted, name = 'Tendencia Lineal', 
            type = 'scatter', mode = 'lines', 
            line = list(color = '#d62728', dash = 'dash'), 
            showlegend = TRUE) %>%
  
  # --- Layout Definitivo (T√≠tulos Combinados) ---
  layout(
    
    # Combinamos T√≠tulo y Subt√≠tulo en un solo bloque
    title = list(
      text = paste0(
        # T√≠tulo (Negrita, Tama√±o 18)
        "<b>Fuerte Ciclo Estacional Sin Tendencia Lineal Significativa</b>",
        
        # Salto de l√≠nea
        "<br>",
        
        # Subt√≠tulo (Tama√±o 12, Color gris)
        "<span style='font-size: 12px; color: gray;'>", 
        "Pendiente calculada: ", 
        sprintf("%.3f ¬∞C/d√©cada (p-valor: %.2f)", slope_decade, pv),
        ". El p-valor alto (p > 0.05) indica que la tendencia es nula.",
        "</span>"
      ),
      
      # --- Par√°metros de Centrado ---
      x = 0.5,            # Posici√≥n horizontal (0.5 = centro)
      xanchor = 'center',   # Anclaje (que el centro del texto est√© en x=0.5)
      y = 0.95,           # Posici√≥n vertical (cerca de la parte superior)
      yanchor = 'top'
    ),
    
    # Ya no usamos 'annotations' para el subt√≠tulo
    
    xaxis = list(title = "Fecha", dtick = "M12", tickformat = "%b %Y"),
    yaxis = list(title = "Temperatura media (¬∞C)"),
    legend = list(title = list(text='<b>Datos</b>')),
    
    # Margen para dar espacio
    margin = list(t = 120) 
  )

p_trend
```

Este tipo de estudios sobre la climatolog√≠a y sus anomal√≠as suelen ser con datos mucho m√°s extensos, de 30 a√±os al menos. Ac√° contamos solo con 5 completos (2018-2023).

Con esa climatolog√≠a base (2018-2023), calculamos las anomal√≠as mensuales (la diferencia de cada mes con su promedio hist√≥rico). El gr√°fico de barras resultante resalta los per√≠odos c√°lidos prolongados en 2023 y un notable evento fr√≠o en 2024.

```{r}
# ================== Promedio diario nacional (plotly) ==================

# --------- Par√°metros ---------
value_col  <- if ("tmax" %in% names(daily_station)) "tmax" else "temp_mean"
plot_start <- as.Date("2018-01-01")
plot_end   <- as.Date("2023-12-31")
norm_years <- 2018:2023

stopifnot(all(c("ymd","year","month", value_col) %in% names(daily_station)))

# --------- 1) Promedio diario nacional ---------
nat_daily <- daily_station %>%
  filter(ymd >= plot_start, ymd <= plot_end) %>%
  group_by(ymd) %>%
  summarise(value = mean(.data[[value_col]], na.rm = TRUE), .groups = "drop") %>%
  mutate(
    year  = year(ymd),
    doy   = yday(ymd),
    is29f = (month(ymd) == 2 & mday(ymd) == 29)
  ) %>%
  filter(!is29f)

# --------- 2) Climatolog√≠a diaria (promedio por DOY) ---------
clim_doy <- nat_daily %>%
  filter(year %in% norm_years) %>%
  group_by(doy) %>%
  summarise(clim = mean(value, na.rm = TRUE), .groups = "drop")

lo <- loess(clim ~ doy, data = clim_doy, span = 0.25, degree = 2, family = "gaussian")
clim_doy <- clim_doy %>%
  mutate(clim_smooth = as.numeric(predict(lo, newdata = data.frame(doy = doy))))


# --------- 3) Unir serie diaria con la climatolog√≠a ---------
plot_df <- nat_daily %>%
  left_join(clim_doy, by = "doy") %>%
  mutate(
    delta = value - clim_smooth,
    signo = if_else(delta >= 0, "sobre", "bajo"),
    signo = factor(signo, levels = c("bajo", "sobre")) # --- NUEVO: Asegurarse que 'signo' sea factor
  )

# --- NUEVO: 3b) Calcular agregados mensuales para la nueva vista ---
monthly_df <- plot_df %>%
  mutate(month_floor = floor_date(ymd, "month")) %>%
  group_by(month_floor) %>%
  summarise(
    delta_mean = mean(delta, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    signo = if_else(delta_mean >= 0, "sobre", "bajo"),
    signo = factor(signo, levels = c("bajo", "sobre")) # --- NUEVO: Asegurarse que 'signo' sea factor
  )

# --- MODIFICADO: Definir rangos de Y para AMBAS vistas ---
# Rango para la vista diaria (temperaturas)
y_rng_daily <- range(c(plot_df$value, plot_df$clim_smooth), na.rm = TRUE) + c(-1, 1)
# Rango para la vista mensual (anomal√≠as)
y_rng_monthly <- range(monthly_df$delta_mean, na.rm = TRUE) + c(-0.5, 0.5)


titulo_var <- if (value_col == "tmax") {
  "Temperatura m√°xima ‚Äî Promedio diario nacional"
} else {
  "Temperatura ‚Äî Promedio diario nacional"
}

# --------- 4) Gr√°fico (plotly) ---------
p_anom <- plot_ly(hoverinfo = "skip") %>%
  
  # --- TRAZA 1: Segmentos diarios (INICIA VISIBLE) ---
  add_segments(
    data = plot_df,
    x = ~ymd,
    y = ~pmin(value, clim_smooth),
    yend = ~pmax(value, clim_smooth),
    xend = ~ymd,
    color = ~signo,
    colors = c("bajo" = "#55C3D3", "sobre" = "#C23BAA"),
    name = "Diferencia Diaria",
    showlegend = FALSE,
    visible = TRUE 
  ) %>%
  
  # --- TRAZA 2: L√≠nea diaria (INICIA VISIBLE) ---
  add_lines(
    data = plot_df,
    x = ~ymd,
    y = ~value,
    name = "Promedio diario nacional",
    line = list(color = "rgba(160,160,160,1)", width = 1),
    hovertemplate = "%{x|%d-%m-%Y}<br>Promedio: %{y:.1f}¬∞C<extra></extra>",
    visible = TRUE 
  ) %>%
  
  # --- TRAZA 3: L√≠nea "Normal" (INICIA VISIBLE) ---
  add_lines(
    data = plot_df,
    x = ~ymd,
    y = ~clim_smooth,
    name = sprintf("Normal diaria %d‚Äì%d (suavizada)", min(norm_years), max(norm_years)),
    line = list(color = "black", width = 2),
    hovertemplate = "%{x|%d-%m-%Y}<br>Normal: %{y:.1f}¬∞C<extra></extra>",
    visible = TRUE 
  ) %>%

  # --- MODIFICADO: TRAZA 4: Barras mensuales (asegura los colores) ---
  add_bars(
    data = monthly_df,
    x = ~month_floor,
    y = ~delta_mean,
    color = ~signo, # Aqu√≠ usa la variable factor 'signo'
    colors = c("bajo" = "#55C3D3", "sobre" = "#C23BAA"), # Y aqu√≠ le indicamos los colores expl√≠citamente para cada nivel
    name = "Anomal√≠a Mensual",
    hovertemplate = "%{x|%Y-%m}<br>Anomal√≠a: %{y:.1f}¬∞C<extra></extra>",
    showlegend = FALSE,
    visible = FALSE
  ) %>%
  
  # --- CORRECCI√ìN DEFINITIVA: Layout con 6 trazas ---
  layout(
    title = list(
      text = paste0(
        titulo_var,
        "<br><sup>Argentina ‚Äî ", format(plot_start, "%Y-%m-%d"),
        " a ", format(plot_end, "%Y-%m-%d"), "</sup>"
      ),
      y = 0.95,
      x = 0.5,
      xanchor = 'center'
    ),
    xaxis = list(title = NULL, dtick = "M3", tickformat = "%b %Y"),
    yaxis = list(title = "Temperatura (¬∞C)", range = y_rng_daily), 
    margin = list(b = 80, t = 120),
    annotations = list(
      list(
        text = sprintf(
          "Normal diaria: promedio %d‚Äì%d (loess). Fuente: SMN ‚Äì elaboraci√≥n propia.",
          min(norm_years), max(norm_years)
        ),
        showarrow = FALSE, xref = "paper", yref = "paper",
        x = 1, y = -0.18, xanchor = "right", font = list(size = 10)
      )
    ),
    
    # --- VECTORES DE 6 ELEMENTOS ---
    updatemenus = list(
      list(
        type = "buttons",
        direction = "right",
        xanchor = 'center',
        yanchor = 'top',
        x = 0.5,
        y = 1.15, 
        buttons = list(
          
          # Bot√≥n 1: Vista Diaria
          list(
            method = "update",
            args = list(
              # T1(seg-), T2(seg+), T3(val), T4(clim), T5(bar-), T6(bar+)
              list(visible = c(TRUE, TRUE, TRUE, TRUE, FALSE, FALSE)),
              list(
                yaxis = list(title = "Temperatura (¬∞C)", range = y_rng_daily),
                title = list(
                  text = paste0(
                    titulo_var,
                    "<br><sup>Argentina ‚Äî ", format(plot_start, "%Y-%m-%d"),
                    " a ", format(plot_end, "%Y-%m-%d"), "</sup>"
                  ),
                  y = 0.95
                )
              )
            ),
            label = "Vista Diaria"
          ),
          
          # Bot√≥n 2: Vista Mensual
          list(
            method = "update",
            args = list(
              # T1(seg-), T2(seg+), T3(val), T4(clim), T5(bar-), T6(bar+)
              list(visible = c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE)),
              list(
                yaxis = list(title = "Anomal√≠a Mensual (¬∞C)", range = y_rng_monthly),
                title = list(
                  text = paste0(
                    "Anomal√≠a mensual de la ", tolower(titulo_var), 
                    "<br><sup>Argentina ‚Äî ", format(plot_start, "%Y-%m-%d"),
                    " a ", format(plot_end, "%Y-%m-%d"), "</sup>"
                  ),
                  y = 0.95
                )
              )
            ),
            label = "Vista Mensual"
          )
        )
      )
    )
  )

p_anom # Vuelve a ejecutar p_anom
# ================== FIN ======================================================
```

Comparamos el ciclo diurno (horario) promedio para el verano y el invierno. El gr√°fico muestra claramente la diferencia en la amplitud t√©rmica diaria y la diferencia de ~12¬∞C entre las estaciones.
```{r}
diurnal <- df %>%
  filter(!is.na(temp)) %>%
  group_by(hora, m = month(datetime)) %>%
  summarise(temp_mean = mean(temp, na.rm = TRUE), .groups = 'drop') %>%
  mutate(
    season = case_when(
      m %in% c(12, 1, 2) ~ "Verano (DJF)",
      m %in% c(6, 7, 8)  ~ "Invierno (JJA)",
      .default = NA_character_ # Asigna NA al resto (oto√±o/primavera)
    )
  ) %>%
  filter(!is.na(season)) %>%
  group_by(hora, season) %>%
  summarise(temp_mean = mean(temp_mean, na.rm = TRUE), .groups = 'drop') %>%
  # Convertir 'season' a factor para ordenar en gr√°ficos
  mutate(
    season = factor(season, levels = c("Verano (DJF)", "Invierno (JJA)"))
  )

p_diurnal <- plot_ly(diurnal, x=~hora, y=~temp_mean, color=~season, type='scatter', mode='lines+markers',
                     colors = c("Invierno (JJA)" = "#1f77b4", "Verano (DJF)" = "#d62728")) %>%
  layout(
    title = list(text = "Temperaturas medias diurnas: Verano es ~12¬∞C m√°s c√°lido que el invierno<br><sup>Promedio horario nacional para Verano (DJF) e Invierno (JJA) del per√≠odo 2018-2024</sup>"),
    
    xaxis = list(title = "Hora local", nticks = 24),
    yaxis = list(title = "Temperatura media (¬∞C)"),
    legend = list(title = list(text = '<b> Estaci√≥n </b>')),
    annotations = list(
      list(text = "Fuente: SMN ‚Äì elaboraci√≥n propia.",
           showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
           xanchor = 'right', yanchor = 'auto', font = list(size = 10))
    ),
    margin = list(b = 80)
  )

p_diurnal
```

# 8. An√°lisis Detallados por Variable
## 8.1. Precipitaci√≥n
Utilizando el conjunto de datos de precipitaci√≥n m√°s largo (1991-2024), analizamos la contribuci√≥n de los eventos de lluvia "fuerte".
Creamos un gr√°fico de barras apiladas que muestra la precipitaci√≥n mensual total (promedio nacional) dividida en dos componentes:
- Azul: Lluvia proveniente de d√≠as "ligeros" (< 50 mm).
- Rojo: Lluvia proveniente de d√≠as "fuertes" (‚â• 50 mm).
Esto visualiza el impacto significativo que tienen los eventos extremos en el total de precipitaci√≥n mensual.

```{r}
# ============= AN√ÅLISIS DE PRECIPITACI√ìN (1991-2024) ============

# 1. Preparaci√≥n de Datos: Convertir y a√±adir bandera de d√≠a fuerte
rain_daily_full <- df_lluvia_con_info |>
  mutate(
    FECHA = as.Date(fecha),
    pp = as.numeric(precipitacion)
  ) |>
  filter(!is.na(nro), !is.na(fecha), !is.na(pp)) |>
  select(nro, fecha, pp) |>
  mutate(
    is_heavy_day = pp >= 50
  )

# 2. Componentes Mensuales por Estaci√≥n: Suma de las componentes
rain_month_station_components <- rain_daily_full |>
  mutate(ym = as.Date(format(fecha, "%Y-%m-01"))) |>
  group_by(nro, ym, is_heavy_day) |>
  summarise(
    pp_component = sum(pp, na.rm = TRUE),
    .groups = 'drop'
  )

# 3. Pivotear y Calcular Promedio Nacional ("Fair")
rain_month_national_stacked_full <- rain_month_station_components |>
  tidyr::pivot_wider(
    names_from = is_heavy_day,
    values_from = pp_component,
    names_prefix = "pp_",
    values_fill = 0
  ) |>
  rename(pp_heavy = pp_TRUE, pp_light = pp_FALSE) |>
  
  # Calcular el Promedio Nacional (M√©todo FAIR)
  group_by(ym) |>
  summarise(
    pp_nat_light = mean(pp_light, na.rm = TRUE),
    pp_nat_heavy = mean(pp_heavy, na.rm = TRUE),
    .groups = 'drop'
  ) |>
  arrange(ym)

# 4. Generaci√≥n del Gr√°fico Apilado (Plotly)

p_rain_full <- plot_ly(rain_month_national_stacked_full, x = ~ym) %>%
    add_trace(
    y = ~pp_nat_heavy,
    type = 'bar',
    name = 'En d√≠as ‚â• 50mm',
    marker = list(color = '#d62728')
  ) %>%
  add_trace(
    y = ~pp_nat_light,
    type = 'bar',
    name = 'En d√≠as < 50mm',
    marker = list(color = '#1f77b4')
  ) %>%
  layout(
    barmode = 'stack',
    title = list(text = "Contribuci√≥n de d√≠as de lluvia fuerte (‚â• 50mm) a la precipitaci√≥n mensual<br><sup>Promedio de acumulados mensuales por estaci√≥n para el periodo 1991‚Äì2024</sup>"),
    xaxis = list(
      title = "Fecha",
      dtick = "M24", # Marcar cada 2 a√±os
      tickformat = "%Y"
    ),
    yaxis = list(title = "Precipitaci√≥n mensual promedio (mm)"),
    
    annotations = list(
      list(text = "Fuente: SMN ‚Äì elaboraci√≥n propia (datos de precipitaci√≥n desde 1991).",
           showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
           xanchor = 'right', yanchor = 'auto', font = list(size = 10))
    ),
    margin = list(b = 80)
  )

p_rain_full
```
En promedio, ¬øcu√°ntos mil√≠metros recibe mensualmente una ubicaci√≥n aleatoria en Argentina de la lluvia que proviene de eventos torrenciales? El tama√±o de la barra roja refleja la rareza del evento, ya que es la la suma de los mm de lluvia fuerte por mes dividio el numero de estaciones. La suma de ambas barras es el promedio de precipitaci√≥n mensual.

A continuaci√≥n, creamos un mapa para visualizar d√≥nde ocurren estos eventos extremos. Los c√≠rculos representan estaciones, con el tama√±o y color indicando la frecuencia de d√≠as con lluvia >= 50 mm. Se observa una clara concentraci√≥n en las regiones NEA/Litoral.
```{r}
# Mapa de eventos de precipitaci√≥n extrema (‚â• 50 mm/d√≠a)
# Lluvia diaria por estaci√≥n-fecha + lat/lon calculados en j
rain_daily <- df %>%
  # Agrupar por estaci√≥n y fecha
  group_by(nro, nombre, provincia, fecha = as.Date(datetime)) %>%
  
  # Calcular precipitaci√≥n diaria total (pp) y obtener coords
  summarise(
    pp = sum(precip_horaria, na.rm = TRUE),
    lat = first(latitud), # Equivalente a latitud[1L]
    lon = first(longitud),# Equivalente a longitud[1L]
    .groups = 'drop'
  )

# ======================================================
# 2. heavy50_day: Filtrar umbral (>= 50 mm/d√≠a) y coords v√°lidas
# ======================================================

heavy50_day <- rain_daily %>%
  filter(pp >= 50 & is.finite(lat) & is.finite(lon))

# ======================================================
# 3. heavy50: Agregar por estaci√≥n (contar eventos y m√°ximo diario)
# ======================================================

heavy50 <- heavy50_day %>%
  # Agrupar por las caracter√≠sticas √∫nicas de la estaci√≥n
  group_by(nro, nombre, provincia) %>%
  
  # Calcular m√©tricas de eventos extremos
  summarise(
    eventos_50mm = n(),           # Equivalente a .N
    max_diario   = max(pp, na.rm = TRUE),
    lat          = first(lat),    # Tomar la primera latitud v√°lida
    lon          = first(lon),    # Tomar la primera longitud v√°lida
    .groups = 'drop'
  )

# ======================================================
# 4. Paleta de color (M√©todo Tidyverse/Leaflet)
# ======================================================

pal_events <- colorNumeric(brewer.pal(9, "Blues"), 
                           domain = heavy50$eventos_50mm)

# Mapa
m_ext <- leaflet(heavy50) %>% addTiles() %>%
  addCircleMarkers(
    lng = ~lon, lat = ~lat,
    radius = ~pmin(14, 4 + scales::rescale(eventos_50mm, to = c(2, 12))),
    color = ~pal_events(eventos_50mm), fillColor = ~pal_events(eventos_50mm),
    fillOpacity = 0.9, stroke = FALSE,
    popup = ~paste0(
      "<b>", nombre, "</b><br>", provincia,
      "<br>Eventos ‚â•50 mm/d√≠a: ", eventos_50mm,
      "<br>M√°x diario: ", round(max_diario, 1), " mm"
    )
  ) %>%
  
  # --- MODIFICACI√ìN 2: T√≠tulo de leyenda m√°s conciso ---
  addLegend("bottomright", pal = pal_events, values = ~eventos_50mm,
            title = "Frecuencia de Eventos") 
  
# --- MODIFICACI√ìN 3: T√≠tulo movido a la esquina superior izquierda (topleft) ---
title_ext_html <- htmltools::tags$div(
  style = "width: 280px;",
  htmltools::tags$h4("Eventos de precipitaci√≥n extrema (‚â• 50 mm/d√≠a)"),
  htmltools::tags$p("Mayor concentraci√≥n de eventos en NEA/Litoral. El tama√±o del punto es proporcional al conteo.", 
                    style = "font-size: small;"),
  htmltools::tags$em("Fuente: SMN ‚Äì elaboraci√≥n propia.", style = "font-size: small;")
)

m_ext <- m_ext %>%
  addControl(title_ext_html, position = "topleft")

m_ext
```


## 8.2. Demanda Energ√©tica (Grados-D√≠a)

Calculamos los Grados-D√≠a de Calefacci√≥n (HDD) y Enfriamiento (CDD). Estas m√©tricas estiman la demanda de energ√≠a.
- HDD (Azul): Mide cu√°nto fr√≠o hizo, indicando la demanda de calefacci√≥n.
- CDD (Rojo): Mide cu√°nto calor hizo, indicando la demanda de refrigeraci√≥n (aire acondicionado).
El gr√°fico de √°reas muestra la clara estacionalidad de la demanda energ√©tica: picos de HDD en invierno y picos de CDD en verano.

```{r}
# --- Definici√≥n de Temperaturas Base Ajustadas ---
T_BASE_CALENTAMIENTO <- 14 # T√≠pica base ajustada para calefacci√≥n (HDD)
T_BASE_ENFRIAMIENTO <- 24 # T√≠pica base para enfriamiento (CDD)

# ======================================================
# 1. Grados-d√≠a DIARIOS por ESTACI√ìN (CON BASES AJUSTADAS)
# ======================================================

dd_station_day_mejorado <- df %>%
  filter(!is.na(temp)) %>%
  group_by(nro, fecha = as.Date(datetime)) %>%
  
  # F√≥rmulas ajustadas con T_BASE_CALENTAMIENTO (16¬∞C) y T_BASE_ENFRIAMIENTO (22¬∞C)
  summarise(
    # HDD: Necesidad de Calefacci√≥n (base 16¬∞C)
    HDD = sum(pmax(0, T_BASE_CALENTAMIENTO - temp)) / 24, 
    
    # CDD: Necesidad de Enfriamiento (base 22¬∞C)
    CDD = sum(pmax(0, temp - T_BASE_ENFRIAMIENTO)) / 24, 
    .groups = 'drop'
  )

# ======================================================
# 2. Acumulado Mensual y 3. Mediana Nacional
# ======================================================

dd_month_national_mejorado <- dd_station_day_mejorado %>%
  
  # 2. Acumulado Mensual por Estaci√≥n
  group_by(
    nro, 
    ym = floor_date(fecha, "month") 
  ) %>%
  summarise(
    HDD_month = sum(HDD, na.rm = TRUE),
    CDD_month = sum(CDD, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  
  # 3. Mediana Nacional "Justa"
  group_by(ym) %>%
  summarise(
    HDD_nat = median(HDD_month, na.rm = TRUE),
    CDD_nat = median(CDD_month, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(ym)

# ---
# 4. Gr√°fico de √ÅREA (La alternativa)
# ---
p_dd <- plot_ly(dd_month_national, x = ~ym) %>%
  
  # Capa de Calefacci√≥n (HDD)
  add_trace(
    y = ~HDD_nat, 
    name = "Calefacci√≥n (HDD)", 
    type = 'scatter', 
    mode = 'lines',
    line = list(color = '#1f77b4'), # Azul
    fill = 'tozeroy',
    fillcolor = 'rgba(31, 119, 180, 0.4)' # Mismo azul, con transparencia
  ) %>%
  
  # Capa de Refrigeraci√≥n (CDD)
  add_trace(
    y = ~CDD_nat, 
    name = "Refrigeraci√≥n (CDD)", 
    type = 'scatter', 
    mode = 'lines',
    line = list(color = '#d62728'), # Rojo/Naranja
    fill = 'tozeroy', # <-- ¬°LA MAGIA EST√Å AQU√ç!
    fillcolor = 'rgba(214, 39, 40, 0.4)' # Mismo rojo, con transparencia
  ) %>%
  
  # --- Layout ---
  layout(
    title = list(text = "La demanda energ√©tica potencial por clima sigue un fuerte ciclo estacional<br><sup>Mediana de Grados-D√≠a (HDD/CDD) por estaci√≥n, umbral 18¬∞C</sup>"),
    xaxis = list(
      title = "Fecha",
      dtick = "M12",
      tickformat = "%b %Y"
    ),
    yaxis = list(title = "Grados-d√≠a acumulados (Mediana)"), 
    legend = list(title = list(text='<b> Demanda </b>')),
    annotations = list(
      list(text = "Fuente: SMN ‚Äì elaboraci√≥n propia.",
           showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
           xanchor = 'right', yanchor = 'auto', font = list(size = 10))
    ),
    margin = list(b = 80)
  )
p_dd
```

## 8.3. Viento (Rosa de Vientos)

Generamos una "Rosa de Vientos" para la estaci√≥n con m√°s registros. Este gr√°fico especializado muestra la frecuencia e intensidad del viento por direcci√≥n. En este ejemplo, se observa un claro predominio de vientos del Norte y Sudeste.
```{r}
# ==================== ROSA DE VIENTOS ========================
# 1. Identificar la estaci√≥n con m√°s datos
st <- df %>%
  count(nombre, sort = TRUE) %>%
  slice(1) %>%
  pull(nombre) # Extraer el valor como un vector simple

# 2. Filtrar el data frame original para obtener los datos de viento
wind_df <- df %>%
  filter(nombre == st) %>%
  select(
    ws = ff,     # Velocidad del viento (wind speed)
    wd = dd,     # Direcci√≥n del viento (wind direction)
    date = datetime # Fecha y hora
  )

cat(sprintf("Rosa de vientos para estaci√≥n %s:\n", st))

# Definici√≥n de los breaks originales
my_breaks <- c(0, 2, 4, 6, 8, 12, 20, 48)

# --- TU NUEVA ESTRATEGIA DE LABELS ---
# Creamos 8 labels (etiquetas), una para CADA punto de break
# (0, 2, 4, 6, 8, 12, 20, y el final 48)
my_key_labels <- c("0", "2", "4", "6", "8", "12", "20", "48")

wind_plot <- openair::windRose(
  wind_df,
  ws = "ws",
  wd = "wd",
  breaks = my_breaks, # Los breaks para el c√°lculo
  paddle = FALSE,
  key.position = "bottom",
  auto.text = FALSE,
  
  main = paste("Predominio de vientos del Norte y Sudeste en la estaci√≥n", as.character(st)),
  sub = "La frecuencia y la intensidad (m/s) definen el patr√≥n local.\nFuente: SMN ‚Äì elaboraci√≥n propia.",

  # --- AJUSTE CLAVE: Usar el argumento 'key' como una lista ---
  # Esto le pasa opciones directamente a la funci√≥n 'draw.colorkey' de lattice
  key = list(
    # 'at' le dice d√≥nde poner los 'ticks' de color (los 8 valores de break)
    at = my_breaks, 
    
    # 'labels' le dice qu√© texto poner en esos 'ticks' (las 8 etiquetas que creamos)
    labels = my_key_labels,
    
    # 'cex' controla el tama√±o de la fuente de los labels de la leyenda
    cex = 0.9, # Reducimos un poco la fuente para que quepa bien
    
    # 'space' controla d√≥nde va la leyenda
    space = "bottom"
  ),
  
  # A openair le gusta tener un key.footer para las unidades
  key.footer = "(m/s)",
  
  # Mantener los ajustes de espacio que arreglaron el t√≠tulo/subt√≠tulo
  par.settings = list(
    layout.heights = list(
      bottom.padding = 4, 
      sub = 4             
    ),
    fontsize = list(text = 9, points = 9) # Mantenemos la fuente legible
  )
)

wind_plot
```

Llevamos esto un paso m√°s all√° y creamos un mapa interactivo de Rosas de Viento.
- El script genera un PNG miniatura de la rosa de vientos para cada estaci√≥n.
- El mapa agrupa estas rosas en cl√∫steres. Al hacer zoom, los cl√∫steres se expanden ("spiderfy").
- Una capa de "Resumen" muestra c√≠rculos simples coloreados por la velocidad media del viento.
- Al hacer clic en una rosa miniatura, aparece una versi√≥n m√°s grande y detallada en el pop-up.

```{r}
set.seed(42) # Para reproducibilidad del clustering

# --- 1) Definici√≥n de Estilo (El que te gust√≥) ---
output_dir <- "wind_roses"
if (!dir.exists(output_dir)) {
  dir.create(output_dir)
}

# Breaks y Labels que definimos (para la leyenda 'key = list')
my_breaks <- c(0, 2, 4, 6, 8, 12, 20, 48)
my_key_labels <- c("0", "2", "4", "6", "8", "12", "20", "48")

# --- 2) Informaci√≥n de Estaciones (dplyr) ---
stations_info <- df %>%
  filter(is.finite(latitud) & is.finite(longitud)) %>%
  group_by(nro, nombre, provincia) %>%
  summarise(
    lat = first(latitud),
    lon = first(longitud),
    .groups = 'drop'
  ) %>%
  filter(is.finite(lat) & is.finite(lon))

# --- 3) Funci√≥n 'make_wind_rose_png' (CORREGIDA) ---

make_wind_rose_png <- function(stn_dt, st_name, file_png, width, height, res,
                               is_thumb = TRUE) {
  
  grDevices::png(filename = file_png, width = width, height = height, res = res, bg = "transparent")
  on.exit(grDevices::dev.off(), add = TRUE)
  
  if (is_thumb) {
    # --- THUMBNAIL (M√©todo robusto) ---
    # Ocultamos todo para el √≠cono
    openair::windRose(
      stn_dt, ws = "ff", wd = "dd",
      breaks = my_breaks,
      paddle = FALSE, 
      auto.text = FALSE,
      key = FALSE,      # Oculta la leyenda
      main = " ",       # Oculta t√≠tulos
      sub = " ",
      xlab = " ",
      ylab = " "
    )
    
  } else {
    # --- FULL PNG (Aplicando la soluci√≥n que pediste) ---
    # (Este es el bloque de c√≥digo exacto de tu ejemplo)
    openair::windRose(
      stn_dt,
      ws = "ff", 
      wd = "dd", 
      breaks = my_breaks,
      paddle = FALSE,
      key.position = "bottom",
      auto.text = FALSE,
      
      main = paste("Rosa de Viento:", st_name),
      sub = "Frecuencia e intensidad (m/s). Fuente: SMN",
      
      # --- AJUSTE CLAVE: Usar el argumento 'key' ---
      key = list(
        at = my_breaks, 
        labels = my_key_labels, # Labels limpios ("0", "2"...)
        cex = 0.9,
        space = "bottom"
      ),
      
      key.footer = "(m/s)",
      
      # Ajustes de m√°rgenes y fuentes
      par.settings = list(
        layout.heights = list(
          bottom.padding = 4, 
          sub = 4
        ),
        fontsize = list(text = 9, points = 9)
      )
    )
  }
}

# --- 4) Bucle de Generaci√≥n (Thumb + Full) ---
png_thumb_list <- list()
png_full_list <- list()
cat("Generando", nrow(stations_info), "pares de rosas de viento en", output_dir, "...\n")

for (i in seq_len(nrow(stations_info))) {
  id      <- stations_info$nro[i]
  st_name <- stations_info$nombre[i]
  
  stn_dt <- df %>%
    filter(nro == id, is.finite(ff), is.finite(dd)) %>%
    select(ff, dd)
  
  if (!nrow(stn_dt)) next
  
  clean_name <- janitor::make_clean_names(st_name)
  f_thumb <- file.path(output_dir, sprintf("%s_thumb.png", clean_name))
  f_full  <- file.path(output_dir, sprintf("%s_full.png",  clean_name))
  
  # Generar PNG miniatura (peque√±o y limpio)
  make_wind_rose_png(stn_dt, st_name, f_thumb, width = 100, height = 100, res = 72, is_thumb = TRUE)
  # Generar PNG completo (M√ÅS GRANDE: 400x400 para que entren t√≠tulos)
  make_wind_rose_png(stn_dt, st_name, f_full,  width = 400, height = 400, res = 96, is_thumb = FALSE)
  
  png_thumb_list[[as.character(id)]] <- f_thumb
  png_full_list[[as.character(id)]] <- f_full
}

# --- 5) Combinar y Codificar en Base64 ---
thumb_df <- tibble(nro_str = names(png_thumb_list), png_thumb = unlist(png_thumb_list))
full_df  <- tibble(nro_str = names(png_full_list),  png_full  = unlist(png_full_list))

map_data <- stations_info %>%
  mutate(nro_str = as.character(nro)) %>%
  left_join(thumb_df, by = "nro_str") %>%
  left_join(full_df,  by = "nro_str") %>%
  select(-nro_str) %>%
  filter(file.exists(png_thumb) & file.exists(png_full))

stopifnot(nrow(map_data) > 0)

map_data <- map_data %>%
  rowwise() %>%
  mutate(
    png_thumb_data = base64enc::dataURI(file = png_thumb, mime = "image/png"),
    png_full_data  = base64enc::dataURI(file = png_full,  mime = "image/png")
  ) %>%
  ungroup()

# --- 6) Capa "resumen" (C√≠rculos) ---
# (CORREGIDO: 'lon' en lugar de 'Lo')
stations_speed <- df %>%
  filter(is.finite(ff)) %>%
  group_by(nro) %>%
  summarise(
    ws_mean = mean(ff, na.rm = TRUE),
    lat = first(latitud), 
    lon = first(longitud), # <-- ¬°Error corregido!
    nombre = first(nombre), 
    provincia = first(provincia),
    .groups = 'drop'
  ) %>%
  filter(is.finite(lat) & is.finite(lon))

pal_ws <- leaflet::colorNumeric("YlGnBu", domain = stations_speed$ws_mean)

# --- 7) Mapa Leaflet (Cl√∫ster + Popup) ---
icon_rose <- leaflet::icons(
  iconUrl   = map_data$png_thumb_data, # El √≠cono es la miniatura limpia
  iconWidth = 72, iconHeight = 72,      # Tama√±o que permite la expansi√≥n (spiderfy)
  className = "rose"
)

m <- leaflet::leaflet() %>%
  leaflet::addProviderTiles(leaflet::providers$CartoDB.Positron) %>%
  
  # Capa Resumen (c√≠rculos)
  leaflet::addCircleMarkers(
    data = stations_speed, group = "Resumen (c√≠rculos)",
    lng = ~lon, lat = ~lat, radius = 6,
    fillColor = ~pal_ws(ws_mean), fillOpacity = 0.8, stroke = FALSE,
    popup = ~paste0("<b>", nombre, "</b><br>", provincia,
                    "<br>Viento medio: ", round(ws_mean, 1), " m/s")
  ) %>%
  leaflet::addLegend("bottomright", pal = pal_ws, values = stations_speed$ws_mean,
                     title = "Velocidad del viento (m/s)", group = "Resumen (c√≠rculos)") %>%
  
  # Capa Detalle (rosas)
  leaflet::addMarkers(
    data = map_data, group = "Rosas (detalle)",
    lng = ~lon, lat = ~lat, icon = icon_rose, # √çcono = thumb
    
    # Popup = full
    popup = ~sprintf("<b>Estacionalidad (%s)</b><br><img src='%s' width='280'>", 
                     nombre, png_full_data),
    
    options = leaflet::markerOptions(zIndexOffset = 1000),
    clusterOptions = leaflet::markerClusterOptions(
      disableClusteringAtZoom = 8, 
      spiderfyOnMaxZoom = TRUE,     # La expansi√≥n FUNCIONAR√Å
      showCoverageOnHover = FALSE,
      maxClusterRadius = 60
    )
  ) %>%
  leaflet::addLayersControl(
    overlayGroups = c("Resumen (c√≠rculos)", "Rosas (detalle)"),
    options = leaflet::layersControlOptions(collapsed = FALSE)
  ) %>%
  leaflet::hideGroup("Rosas (detalle)")

# Mostrar el mapa
m
```

# 9. Dispersi√≥n y Modelado
## 9.1. Dispersi√≥n entre Estaciones

Este "gr√°fico de cinta" (ribbon plot) muestra la serie temporal nacional, pero en lugar de solo la media, grafica la mediana (P50) y el rango del 80% (P10-P90) de todas las estaciones.

La cinta (√°rea sombreada) representa la dispersi√≥n o variabilidad entre las diferentes regiones del pa√≠s. Se puede ver que la variabilidad es mayor en invierno (cinta m√°s ancha).
```{r}
# Ribbons de percentiles nacionales (dispersi√≥n entre estaciones)
nat_percentiles <- monthly_by_station_fair %>%
  group_by(ym) %>%
  summarise(
    # La columna 'y' contiene la temperatura media mensual por estaci√≥n
    p10 = quantile(y, 0.10, na.rm = TRUE),
    p50 = quantile(y, 0.50, na.rm = TRUE),
    p90 = quantile(y, 0.90, na.rm = TRUE),
    n_est = n(), 
    .groups = 'drop'
  ) %>%
  # Ordenar por fecha
  arrange(ym)

p_ribbon <- plot_ly(nat_percentiles, x=~ym) %>%
    
    # 1. Banda de Variabilidad (p10-p90)
    add_ribbons(ymin = ~p10, ymax = ~p90, 
                name = "Rango 80% (p10‚Äìp90)", 
                opacity = 0.25,
                line = list(color = 'transparent'), 
                fillcolor = 'lightblue') %>% 
    
    # 2. L√≠nea Central (Mediana)
    add_trace(y=~p50, name="Mediana (P50)", mode="lines",
              line = list(color = '#1f77b4', width = 2)) %>%
    
    # --- Layout Final Integrando el An√°lisis de Sesgo ---
    layout(
        # T√≠tulo: El t√≠tulo principal se mantiene descriptivo.
        title = list(text = "Temperatura nacional: un ciclo estacional constante con alta amplitud anual<br>
        <sup><b>La mediana (P50) se acerca al l√≠mite superior (P90) en verano, indicando un fuerte sesgo hacia condiciones c√°lidas en la mayor√≠a de las estaciones.</b></sup>"),
        
        # Eje X 
        xaxis = list(
            title = "Fecha",
            dtick = "M12", 
            tickformat = "%b %Y"
        ),
        
        # Eje Y
        yaxis = list(title="Temperatura Mensual (¬∞C)"), 
        
        # Leyenda
        legend = list(title = list(text = '<b>M√©tricas</b>'),
                      x = 100,
                      y = 1),
        
        # Fuente
        annotations = list(
            list(text = "Fuente: SMN ‚Äì elaboraci√≥n propia.",
                 showarrow = FALSE, xref = "paper", yref = "paper", x = 1, y = -0.15,
                 xanchor = 'right', yanchor = 'auto', font = list(size = 10))
        ),
        
        margin = list(b = 80, t = 80)
    )

p_ribbon
```


## 9.2. Modelado y Pron√≥stico (Prophet)

Como experimento final, intentamos predecir datos meteorol√≥gicos usando el modelo prophet de Facebook / Meta.

Primero, intentamos pronosticar la precipitaci√≥n mensual para la estaci√≥n con m√°s datos (1991-2024). Entrenamos con datos hasta 2017 y probamos en 2018-adelante. El modelo captura la estacionalidad general.

```{r}
# --- PASO 1: Cargar y Preparar los Datos ---
df_lluvia <- df_lluvia_con_info

# Limpiamos los datos
df_limpio <- df_lluvia %>%
  mutate(
    fecha = as.Date(fecha),
    precipitacion = as.numeric(precipitacion)
  )

# --- PASO 2: Encontrar la Mejor Estaci√≥n (con m√°s datos v√°lidos) ---
estacion_con_mas_datos <- df_limpio %>%
  group_by(nombre) %>%
  summarise(
    registros_validos = sum(!is.na(precipitacion))
  ) %>%
  # Ordenamos de mayor a menor
  arrange(desc(registros_validos)) %>%
  # Elegimos la primera (la mejor)
  slice(1) %>%
  pull(nombre) # Extraemos el nombre

print(paste("Estaci√≥n seleccionada:", estacion_con_mas_datos))

# --- PASO 3: Preparar Datos para Prophet (Filtrar y Agregar) ---
df_prophet <- df_limpio %>%
  filter(nombre == estacion_con_mas_datos) %>%
  mutate(mes_ano = floor_date(fecha, "month")) %>%
  group_by(mes_ano) %>%
  summarise(
    precipitacion_total = sum(precipitacion, na.rm = TRUE)
  ) %>%
  # Renombramos para que Prophet entienda
  rename(
    ds = mes_ano,
    y = precipitacion_total
  ) %>%
  ungroup()

# --- PASO 4: Dividir Datos (Train y Test) ---
# Train: 1991-01-01 hasta 2017-12-31
# Test: 2018-01-01 hasta el final

df_train <- df_prophet %>%
  filter(ds <= as.Date("2017-12-31"))

df_test <- df_prophet %>%
  filter(ds > as.Date("2017-12-31"))

print(paste("Registros de Entrenamiento:", nrow(df_train)))
print(paste("Registros de Prueba:", nrow(df_test)))

# --- PASO 5: Crear y Entrenar el Modelo Prophet ---

# Iniciamos Prophet.
# Prophet detectar√° autom√°ticamente la estacionalidad anual (yearly.seasonality)
# ya que tenemos datos de varios a√±os.
model <- prophet()

# Ajustamos el modelo (entrenamos)
model <- fit.prophet(model, df_train)

# --- PASO 6: Predecir y Evaluar ---
future_test <- df_test %>% select(ds)

# Realizamos la predicci√≥n
forecast <- predict(model, future_test)

# Unimos la predicci√≥n (forecast) con los valores reales (df_test)
# para poder comparar
df_comparacion <- forecast %>%
  select(ds, yhat, yhat_lower, yhat_upper) %>%
  left_join(df_test, by = "ds")

# --- PASO 7: Visualizar los Resultados ---

# 1. Gr√°fico completo de Prophet (Predicci√≥n vs Real)
# Esto crear√° un gr√°fico interactivo si est√°s en RStudio
plot_forecast <- plot(model, forecast)
plot_forecast <- plotly::ggplotly(plot_forecast) # <-- A√ëADE ESTA L√çNEA
plot_forecast

# 2. Gr√°fico de los componentes del modelo
# Esto te mostrar√° la tendencia, la estacionalidad anual, etc.
plot_forecast_comp <- prophet_plot_components(model, forecast)
plot_forecast_comp
```

Repetimos el proceso para la temperatura diaria, que es una se√±al mucho m√°s estable. Entrenamos el modelo en la estaci√≥n con m√°s datos de temperatura hasta finales de 2023 y le pedimos que prediga el futuro (el "test set").
```{r}
# --- PASO 2: Encontrar la Mejor Estaci√≥n (con m√°s datos de TEMPERATURA) ---
estacion_con_mas_datos_temp <- df %>%
  filter(!is.na(temp)) %>% # Filtramos NAs de temperatura
  group_by(nombre) %>%
  summarise(registros_validos = n()) %>% # Contamos las filas restantes
  arrange(desc(registros_validos)) %>%
  slice(1) %>%
  pull(nombre) 

print(paste("Estaci√≥n seleccionada (con m√°s datos de temp):", estacion_con_mas_datos_temp))

# --- PASO 3: Preparar Datos para Prophet (AGREGACI√ìN DIARIA) ---
df_prophet_diario <- df %>%
  filter(nombre == estacion_con_mas_datos_temp) %>%
  group_by(fecha) %>%
  summarise(
    temp_media_diaria = mean(temp, na.rm = TRUE)
  ) %>%
  rename(
    ds = fecha,
    y = temp_media_diaria
  ) %>%
  ungroup() %>%
  distinct() # Nos aseguramos de tener un solo valor por d√≠a

# --- PASO 4: Dividir Datos (Train y Test) ---
fecha_corte <- as.Date("2023-10-07")

df_train_diario <- df_prophet_diario %>%
  filter(ds <= fecha_corte)

df_test_diario <- df_prophet_diario %>%
  filter(ds > fecha_corte)

print(paste("Registros Diarios de Entrenamiento:", nrow(df_train_diario)))
print(paste("Registros Diarios de Prueba:", nrow(df_test_diario)))

# --- PASO 5: Crear y Entrenar el Modelo Prophet ---
# Prophet detectar√° autom√°ticamente la estacionalidad ANUAL y SEMANAL
model_temp_diario <- prophet(weekly.seasonality = FALSE)

# Ajustamos el modelo (entrenamos)
model_temp_diario <- fit.prophet(model_temp_diario, df_train_diario)

# --- PASO 6: Predecir y Evaluar ---

# Creamos un dataframe "futuro"
future_test_diario <- df_test_diario %>% select(ds)

# Realizamos la predicci√≥n
forecast_temp_diario <- predict(model_temp_diario, future_test_diario)

# --- PASO 7: Visualizar los Resultados ---

# 1. Gr√°fico completo de Prophet (Predicci√≥n vs Real)
# Esto tardar√° un poquito m√°s en generarse, ¬°tiene muchos puntos!
p_main <- plot(model_temp_diario, forecast_temp_diario)
p_main <- plotly::ggplotly(p_main)
p_main
# 2. Gr√°fico de los componentes del modelo
# Ahora ver√°s una estacionalidad "semanal" (weekly) adem√°s de la anual
p_comp <- prophet_plot_components(model_temp_diario, forecast_temp_diario)
p_comp
```

El gr√°fico de componentes (p_comp) descompone el pron√≥stico del modelo, mostrando la fuerte tendencia anual y la tendencia semanal (patrones de d√≠as de la semana).

El gr√°fico de comparaci√≥n (pr_comp) muestra que la predicci√≥n del modelo (azul) sigue muy de cerca a los valores reales (negro) en el conjunto de prueba.

```{r}
# --- PASO 8: Comparar Predicci√≥n Diaria vs. Realidad (Gr√°fico Final) ---

# 1. Unimos las predicciones con los datos reales del Test Set (Diario)
df_comparacion_diario <- forecast_temp_diario %>%
  select(ds, yhat, yhat_lower, yhat_upper) %>%
  left_join(df_test_diario, by = "ds") # 'df_test_diario' tiene el valor real 'y'

# 2. Graficamos la comparaci√≥n usando ggplot2
# (Aseg√∫rate de tener 'library(ggplot2)' cargada al inicio de tu Rmd)

pr_comp <- ggplot(df_comparacion_diario, aes(x = ds)) +
  
  # L√≠nea Negra: Los valores REALES diarios que guardamos
  geom_line(aes(y = y, color = "Valor Real"), linewidth = 0.8) +
  
  # L√≠nea Azul Punteada: La PREDICCI√ìN diaria del modelo
  geom_line(aes(y = yhat, color = "Predicci√≥n"), linetype = "dashed", linewidth = 1) +
  
  # √Årea Gris de Incertidumbre
  geom_ribbon(aes(ymin = yhat_lower, ymax = yhat_upper), fill = "grey", alpha = 0.3) +
  
  # T√≠tulos y colores
  scale_color_manual(values = c("Valor Real" = "black", "Predicci√≥n" = "dodgerblue")) +
  labs(title = "Comparaci√≥n: Temperatura Diaria Real vs. Predicci√≥n (Test Set)",
       subtitle = "El modelo sigue bien la tendencia general y la estacionalidad.",
       y = "Temperatura Media Diaria (¬∞C)",
       x = "Fecha",
       color = "Leyenda") +
  theme_minimal()
pr_comp <- plotly::ggplotly(pr_comp)
pr_comp
```

Calculamos el error num√©rico:
- MAE (Error Absoluto Medio): En promedio, el modelo se equivoca por esta cantidad de grados.
- RMSE (Ra√≠z del Error Cuadr√°tico Medio): Similar al MAE, pero penaliza m√°s los errores grandes.

```{r}
# --- PASO 9: Evaluaci√≥n Num√©rica (MAE y RMSE) ---

# 1. Calculamos el error de cada d√≠a
df_errores <- df_comparacion_diario %>%
  mutate(
    error = y - yhat,           # Error (diferencia simple)
    error_absoluto = abs(error) # Error absoluto (siempre positivo)
  )

# 2. Calculamos el MAE (Mean Absolute Error)
# Es el promedio de los errores absolutos
mae <- mean(df_errores$error_absoluto, na.rm = TRUE)

print(paste("MAE (Error Absoluto Medio):", round(mae, 2), "grados"))
print("Esto significa que, en promedio, el modelo se equivoc√≥ por esta cantidad de grados.")

# 3. Calculamos el RMSE (Root Mean Squared Error)
# Es la ra√≠z cuadrada del promedio de los errores al cuadrado
rmse <- sqrt(mean(df_errores$error^2, na.rm = TRUE))

print(paste("RMSE (Ra√≠z del Error Cuadr√°tico Medio):", round(rmse, 2), "grados"))
print("Similar al MAE, pero penaliza m√°s los errores grandes.")
```

Finalmente, analizamos los residuos (los errores del modelo). El gr√°fico de residuos en el tiempo (p_res) debe parecer ruido aleatorio centrado en 0.
```{r}
# --- PASO 10 (A): Gr√°fico de Residuos a lo largo del tiempo ---

# df_errores ya tiene la columna 'error' (y - yhat) y 'ds' (fecha)
p_res <- ggplot(df_errores, aes(x = ds, y = error)) +
  geom_line(color = "red", alpha = 0.8) +
  
  # L√≠nea de referencia en 0 (el error perfecto)
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue", linewidth = 1) +
  
  labs(title = "Residuos (Errores) a lo largo del tiempo",
       subtitle = "Idealmente: Ruido aleatorio centrado en 0",
       y = "Error (Real - Predicci√≥n en ¬∞C)",
       x = "Fecha") +
  theme_minimal()
p_res <- plotly::ggplotly(p_res)
p_res
```
```{r}
# --- PASO 10 (B): Histograma de Residuos ---

hist_res <- ggplot(df_errores, aes(x = error)) +
  
  # Histograma con un color de relleno m√°s agradable
  geom_histogram(aes(y = ..density..), 
                 bins = 40, # Aumentamos un poco los bins
                 fill = "deepskyblue3", 
                 color = "white", # Borde blanco para separar las barras
                 alpha = 0.8) +
  
  # Curva de densidad para ver la forma general
  geom_density(color = "black", linewidth = 1.2, alpha = 0.9) +
  
  # L√≠nea vertical en Cero (Error Perfecto)
  geom_vline(xintercept = 0, 
             color = "black", 
             linetype = "dashed", 
             linewidth = 1.2) +
  
  # L√≠neas verticales para el Error Absoluto Medio (MAE)
  # Esto nos da un contexto de qu√© tan grande es el error promedio
  geom_vline(xintercept = mae, 
             color = "red", 
             linetype = "dotted", 
             linewidth = 1) +
  geom_vline(xintercept = -mae, 
             color = "red", 
             linetype = "dotted", 
             linewidth = 1) +
  
  # T√≠tulos y etiquetas mejorados
  labs(
    title = "Distribuci√≥n de Errores de Predicci√≥n",
    subtitle = paste0("La mayor√≠a de los errores se centran en 0.\nLas l√≠neas rojas punteadas marcan el Error Absoluto Medio (MAE: +/- ", round(mae, 2), "¬∞)"),
    x = "Error de Predicci√≥n (Grados ¬∞C)",
    y = "Densidad"
  ) +
  
  # Un tema limpio
  theme_minimal()
hist_res <- plotly::ggplotly(hist_res)
hist_res
```
El histograma de residuos (hist_res) muestra que la gran mayor√≠a de los errores son muy cercanos a cero, con algunos errores m√°s grandes en las colas. Esto confirma un buen ajuste del modelo.

# 10. Salida de Datos y Gr√°ficos
## Descarga de dataframes para construir graficos en la shiny app
En este bloque final, preparamos los datos para ser consumidos por una aplicaci√≥n externa (como una Shiny app). Normalizamos los dataframes clave (como feat_month, daily_station) y los guardamos en formato .Rds.
```{r}
# --- 0) Normalizaci√≥n de df ---------------------------------------------------
# df ya existe en memoria
df <- as.data.table(df)
setnames(df, make_clean_names(names(df)))

# Tipos (sin :=, usando set())
# fecha
df[, fecha_chr := as.character(fecha)]
set(df, j = "fecha", value = as.Date(df[["fecha_chr"]]))
df[, fecha_chr := NULL]

# hora, nro
set(df, j = "hora", value = as.integer(df[["hora"]]))
set(df, j = "nro",  value = as.integer(df[["nro"]]))

# lat/long
set(df, j = "latitud",  value = as.numeric(df[["latitud"]]))
set(df, j = "longitud", value = as.numeric(df[["longitud"]]))

# datetime local BA
set(df, j = "datetime",
    value = as.POSIXct(paste(df[["fecha"]], sprintf("%02d:00:00", df[["hora"]])),
                       tz = "America/Argentina/Buenos_Aires"))

if (!dir.exists("data")) {
  dir.create("data")
}
saveRDS(df, "data/df_clean")
saveRDS(daily_station, "data/daily_station")
saveRDS(monthly_by_station_fair, "data/monthly_by_station")

# --- 1) GDD mensual (base 10¬∞C) ----------------------------------------------
gdd_daily <- df[!is.na(temp),
                .(gdd = sum(pmax(0, temp - 10))/24),
                by = .(nro, ymd = as.Date(datetime))]
gdd_month <- gdd_daily[, .(gdd_m = sum(gdd)),
                       by = .(nro, ym = as.Date(format(ymd, "%Y-%m-01")))]

saveRDS(gdd_month, "data/gdd_month")

# --- 2) Join de estaciones (Nro -> Nombre) -----------------------------------
suppressPackageStartupMessages({
  library(data.table); library(janitor)
})

# 2.1 Cargar CSV y normalizar nombres
est_csv <- fread("smn_estaciones.csv", encoding = "UTF-8")
setnames(est_csv, make_clean_names(names(est_csv)))  # 'Nro'->'nro', 'Nombre'->'nombre'

# 2.2 Tipos y validaciones
if (!"nro" %in% names(est_csv)) stop("El CSV no tiene columna 'Nro' (o 'nro' tras normalizar).")
if (!"nombre" %in% names(est_csv)) stop("El CSV no tiene columna 'Nombre' (o 'nombre' tras normalizar).")
est_csv[, nro := as.integer(nro)]
est_csv[, nombre := as.character(nombre)]

# 2.3 Resolver duplicados por nro: quedarnos con el nombre m√°s frecuente
est_map <- est_csv[!is.na(nro) & !is.na(nombre),
                   .N, by = .(nro, nombre)][order(nro, -N)][, .SD[1], by = nro][, N := NULL]

# 2.4 Asegurar tipos en gdd_month y agregar nombre_ref
stopifnot(exists("gdd_month"))
setnames(gdd_month, make_clean_names(names(gdd_month)))
if (!"nro" %in% names(gdd_month)) stop("gdd_month no tiene columna 'nro'.")  # corregido el typo
gdd_month[, nro := as.integer(nro)]

gdd_month_joined <- merge(
  gdd_month,
  est_map[, .(nro, nombre_ref = nombre)],
  by = "nro", all.x = TRUE
)

# --- 3) Preparar monthly_by_station (tiene nombre, ds, y) --------------------
monthly_by_station <- as.data.table(monthly_by_station_fair)
setnames(monthly_by_station, make_clean_names(names(monthly_by_station)))  # nombre, ds, y

# Validaciones m√≠nimas
if (!all(c("nombre","ds") %in% names(monthly_by_station))) {
  stop("monthly_by_station debe tener columnas 'nombre' y 'ds'.")
}
monthly_by_station[, nombre := as.character(nombre)]
monthly_by_station[, ds := as.character(ds)]

# ds puede venir como "YYYY-MM" o "YYYY-MM-DD": normalizo y creo ym = primer d√≠a de mes
monthly_by_station[, ds := ifelse(grepl("^\\d{4}-\\d{2}$", ds), paste0(ds, "-01"), ds)]
monthly_by_station[, ym := as.Date(ds)]
monthly_by_station[, ym := as.Date(format(ym, "%Y-%m-01"))]

# --- 4) Agregar GDD por nombre + mes -----------------------------------------
# Normalizo gdd_month_joined y agrego por nombre_ref + ym
setnames(gdd_month_joined, make_clean_names(names(gdd_month_joined)))
if (!all(c("nombre_ref","ym","gdd_m") %in% names(gdd_month_joined))) {
  stop("gdd_month_joined debe tener 'nombre_ref', 'ym' y 'gdd_m'.")
}
gdd_month_joined[, nombre_ref := as.character(nombre_ref)]
gdd_month_joined[, ym := as.Date(ym)]

# Si existiera m√°s de un nro con el mismo nombre en un mes, sumo gdd_m
gdd_by_nombre <- gdd_month_joined[!is.na(nombre_ref) & !is.na(ym),
                                  .(gdd_m = sum(gdd_m, na.rm = TRUE)),
                                  by = .(nombre = nombre_ref, ym)]

# --- 5) Features integradas: merge por nombre + ym ---------------------------
gdd_month_joined <- unique(gdd_month_joined, by = "nro")
monthly_by_station <- unique(monthly_by_station, by = "nro")
feat_month <- merge(
  monthly_by_station,
  gdd_month_joined,
  by = "nro",
  all.x = TRUE
)
# Orden sugerido si esas columnas existen
cols_target <- c("nombre","ym","y","ds","gdd_m","latitud","longitud","provincia",
                 "temp_m","hum_m","ff_m","pnm_m","pp_m","ndays")
setcolorder(feat_month, c(intersect(cols_target, names(feat_month)),
                          setdiff(names(feat_month), cols_target)))

saveRDS(feat_month, "data/feat_month")

# --- 6) Choices b√°sicos para la app ------------------------------------------
estaciones <- sort(unique(na.omit(feat_month$nombre)))
anios      <- sort(unique(year(feat_month$ym)))  # a√±os presentes en los datos mensuales
saveRDS(estaciones, "data/estaciones")
saveRDS(anios,      "data/anios")

message("=== Listo. Merge por nombre + ym; sin requerir nro en monthly_by_station ===")

```

## Descarga de gr√°ficos en memoria para exponer

Por √∫ltimo, definimos una funci√≥n de ayuda (save_all_plots) que recorre el entorno de R, encuentra todos los objetos de gr√°ficos (ggplot, plotly, leaflet, trellis) que creamos durante este an√°lisis y los guarda autom√°ticamente en disco (como PNG, PDF, HTML) en la carpeta figs/.

Esto nos permite exportar f√°cilmente todas las visualizaciones en la propia shiny app.

```{r}
save_all_plots <- function(
  dir_out = "figs",
  formats = c("png","pdf","svg"),
  width = 7, height = 5, dpi = 300,
  env = .GlobalEnv
) {
  dir.create(dir_out, recursive = TRUE, showWarnings = FALSE)

  objs <- ls(envir = env, all.names = TRUE)
  n_saved <- 0

  for (nm in objs) {
    obj <- get(nm, envir = env)

    # --- ggplot2 --------------------------------------------------------------
    if (inherits(obj, "ggplot")) {
      for (fmt in formats) {
        f <- file.path(dir_out, paste0(nm, ".", fmt))
        try({
          ggplot2::ggsave(
            filename = f, plot = obj,
            width = width, height = height, dpi = dpi, units = "in", device = fmt
          )
          n_saved <- n_saved + 1
        }, silent = TRUE)
      }
      next
    }

    # --- htmlwidgets: plotly/leaflet/highcharter ------------------------------
    if (inherits(obj, "htmlwidget")) {
      f <- file.path(dir_out, paste0(nm, ".html"))
      try({
        htmlwidgets::saveWidget(obj, file = f, selfcontained = TRUE)
        n_saved <- n_saved + 1
      }, silent = TRUE)
      next
    }

    # --- lattice/trellis ------------------------------------------------------
    if (inherits(obj, "trellis")) {
      for (fmt in formats) {
        f <- file.path(dir_out, paste0(nm, ".", fmt))
        try({
          switch(fmt,
            png = {
              png(f, width = width, height = height, units = "in", res = dpi)
              print(obj); dev.off()
            },
            pdf = {
              pdf(f, width = width, height = height)
              print(obj); dev.off()
            },
            svg = {
              svg(f, width = width, height = height)
              print(obj); dev.off()
            },
            { } # otros formatos se pueden agregar ac√°
          )
          n_saved <- n_saved + 1
        }, silent = TRUE)
      }
      next
    }

    # --- gr√°ficos base guardados con recordPlot() -----------------------------
    if (inherits(obj, "recordedplot")) {
      f_png <- file.path(dir_out, paste0(nm, ".png"))
      try({
        png(f_png, width = width, height = height, units = "in", res = dpi)
        replayPlot(obj)
        dev.off()
        n_saved <- n_saved + 1
      }, silent = TRUE)
      next
    }
  }

  message(sprintf("Listo: guardados %d archivos en '%s'.", n_saved, dir_out))
}

# Ejecut√°:
save_all_plots(dir_out = "figs", formats = c("png","pdf","svg"), width = 7, height = 5, dpi = 300)

```
